{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"UNET ER 2020_22_01_CombinedLoss 4Class.ipynb","provenance":[{"file_id":"1txy9g7gYnELmPTY6sT4JtfbhwQthIyoG","timestamp":1583812009231},{"file_id":"1a9BwVwMfQ4iFdte5xgs4kzj58JNqs4Vu","timestamp":1579539718864},{"file_id":"1KyLESiefS0uSAi_kTOAZ9NmNUSYTs9lL","timestamp":1578995412588},{"file_id":"1YDTADQrLw8-aYCSjOEYxDuIB1nuy3Xzm","timestamp":1577686452735},{"file_id":"1DqrMpMvIbzPtWFjsL_eNwrVM-xaFvx6c","timestamp":1571727299009}],"collapsed_sections":[],"machine_shape":"hm"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ixvBWa23MB7e","colab_type":"code","outputId":"082a461f-85ba-4155-d65b-d317cd6ee5b0","executionInfo":{"status":"ok","timestamp":1586236654116,"user_tz":-330,"elapsed":1697,"user":{"displayName":"Ramya B","photoUrl":"","userId":"10194325446164447523"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LzNbXYWE4I5E","colab_type":"code","colab":{}},"source":["import os\n","import cv2\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rdhsO-GqAzfp","colab_type":"code","colab":{}},"source":["# Function to convert image from greyscale to one hot (i.e. 3 channels)\n","def convertToOneHot(image):\n","    num_classes = 4\n","    shape = image.shape[:2]+(num_classes,)\n","    encoded_image = np.zeros(shape, dtype=np.uint8)\n","    for r in range(len(image)):\n","      for c in range(len(image[0])):\n","        if image[r][c]==0:\n","          encoded_image[r][c][0] = 1\n","        elif image[r][c]==85:\n","          encoded_image[r][c][1] = 1\n","        elif image[r][c]==170:\n","          encoded_image[r][c][2] = 1\n","        else:\n","          encoded_image[r][c][3] = 1\n","    return encoded_image"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-iWVfAlaM-95","colab_type":"code","colab":{}},"source":["# Function to convert back from one hot image to greyscale\n","def convertFromOneHot(image):\n","    shape = image.shape[:2]\n","    encoded_image = np.zeros(shape, dtype=np.uint8)\n","    for r in range(len(image)):\n","      for c in range(len(image[0])):\n","        if image[r][c][0] == 1:\n","          encoded_image[r][c] = 0\n","        elif image[r][c][1] == 1:\n","          encoded_image[r][c] = 85\n","        elif image[r][c][2] == 1:\n","          encoded_image[r][c] = 170\n","        else:\n","          encoded_image[r][c] = 255\n","    return encoded_image"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xiXrY-SjvEMW","colab_type":"code","colab":{}},"source":["PATH = \"/content/drive/My Drive/Breast Cancer Treatment/Numpy Arrays/4 Class ER\"\n","\n","# Train set patients: 230 232 239 242 \n","TrainX_1 = np.load(PATH + '/ER IHC 263 Images.npy')\n","TrainY_1 = np.load(PATH + '/ER IHC 263 Masks.npy')\n","TrainX_2 = np.load(PATH + '/ER IHC 232 Images.npy')\n","TrainY_2 = np.load(PATH + '/ER IHC 232 Masks.npy')\n","TrainX_3 = np.load(PATH + '/ER IHC 242 Images.npy')\n","TrainY_3 = np.load(PATH + '/ER IHC 242 Masks.npy')\n","TrainX_4 = np.load(PATH + '/ER IHC 230 Images.npy')\n","TrainY_4 = np.load(PATH + '/ER IHC 230 Masks.npy')\n","\n","# Test set patients: 221 246 263\n","TestX_1 = np.load(PATH + '/ER IHC 221 Images.npy')\n","TestY_1 = np.load(PATH + '/ER IHC 221 Masks.npy')\n","TestX_2 = np.load(PATH + '/ER IHC 246 Images.npy')\n","TestY_2 = np.load(PATH + '/ER IHC 246 Masks.npy')\n","TestX_3 = np.load(PATH + '/ER IHC 239 Images.npy')\n","TestY_3 = np.load(PATH + '/ER IHC 239 Masks.npy')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FAF_4OPawvpc","colab_type":"code","outputId":"6c7d8db8-5bd6-43ba-c3aa-2da51fbf5868","executionInfo":{"status":"ok","timestamp":1586236855911,"user_tz":-330,"elapsed":202684,"user":{"displayName":"Ramya B","photoUrl":"","userId":"10194325446164447523"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["TrainX = np.concatenate((TrainX_1, TrainX_2, TrainX_3, TrainX_4), axis=0)\n","TrainGroundTruth = np.concatenate((TrainY_1, TrainY_2, TrainY_3, TrainY_4), axis=0)\n","TrainY = []\n","\n","for img in TrainGroundTruth:\n","  encoded_img = convertToOneHot(img)\n","  TrainY.append(encoded_img)\n","\n","TrainY = np.asarray(TrainY)\n","\n","TestX = np.concatenate((TestX_1, TestX_2, TestX_3), axis=0)\n","TestGroundTruth = np.concatenate((TestY_1, TestY_2, TestY_3), axis=0)\n","TestY = []\n","\n","for img in TestGroundTruth:\n","  encoded_img = convertToOneHot(img)\n","  TestY.append(encoded_img)\n","\n","TestY = np.asarray(TestY)\n","\n","ValidX = TrainX[1800:, : ]\n","ValidY = TrainY[1800:, : ]\n","\n","TrainX = TrainX[0:1800, : ]\n","TrainY = TrainY[0:1800, : ]\n","\n","print(TrainX.shape)\n","print(TrainY.shape)\n","print(TestX.shape)\n","print(TestY.shape)\n","print(ValidX.shape)\n","print(ValidY.shape)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["(1800, 240, 240, 3)\n","(1800, 240, 240, 4)\n","(1440, 240, 240, 3)\n","(1440, 240, 240, 4)\n","(120, 240, 240, 3)\n","(120, 240, 240, 4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HjHNZ_ZfMB8K","colab_type":"code","colab":{}},"source":["TrainX = np.reshape(TrainX,(-1,240, 240,3))\n","TrainY = np.reshape(TrainY,(-1,240, 240,4))\n","\n","TrainX = TrainX.astype('float32')/255\n","\n","TestX = np.reshape(TestX,(-1,240, 240,3))\n","TestY = np.reshape(TestY,(-1,240, 240,4))\n","\n","TestX = TestX.astype('float32')/255\n","\n","ValidX = np.reshape(ValidX,(-1,240, 240,3))\n","ValidY = np.reshape(ValidY,(-1,240, 240,4))\n","\n","ValidX = ValidX.astype('float32')/255"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FG7FXY3fMB8P","colab_type":"code","outputId":"bcd7dbb5-c1fb-446b-8a9d-f363e331c8d1","executionInfo":{"status":"ok","timestamp":1586236856535,"user_tz":-330,"elapsed":202321,"user":{"displayName":"Ramya B","photoUrl":"","userId":"10194325446164447523"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["print(TrainX.shape)\n","print(TrainY.shape)\n","\n","print(TestX.shape)\n","print(TestY.shape)\n","\n","print(ValidX.shape)\n","print(ValidY.shape)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["(1800, 240, 240, 3)\n","(1800, 240, 240, 4)\n","(1440, 240, 240, 3)\n","(1440, 240, 240, 4)\n","(120, 240, 240, 3)\n","(120, 240, 240, 4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JPrbHkJDMB8e","colab_type":"code","outputId":"866528d8-d937-46c1-ff18-a1a8d818ae55","executionInfo":{"status":"ok","timestamp":1586236865515,"user_tz":-330,"elapsed":210743,"user":{"displayName":"Ramya B","photoUrl":"","userId":"10194325446164447523"}},"colab":{"base_uri":"https://localhost:8080/","height":916}},"source":["!pip install medpy\n","from keras.layers import *\n","import keras\n","from keras.models import Sequential\n","import cv2\n","import os\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from medpy.metric import dc, precision, recall\n","from keras import Model\n","!pip install tensorflow==1.14.0"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: medpy in /usr/local/lib/python3.6/dist-packages (0.4.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from medpy) (1.4.1)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from medpy) (1.18.2)\n","Requirement already satisfied: SimpleITK>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from medpy) (1.2.4)\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Requirement already satisfied: tensorflow==1.14.0 in /usr/local/lib/python3.6/dist-packages (1.14.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.9.0)\n","Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.14.0)\n","Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.14.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.27.2)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.34.2)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.8.1)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.2.0)\n","Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.18.2)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.0.8)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (3.10.0)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.3.3)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.2.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (46.1.3)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.10.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"U4SPHrcLZH4a","colab":{}},"source":["def get_conv_block(input_layer,nFilters,size):\n","    conv1 = Conv2D(nFilters, size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(input_layer)\n","    bn1 = BatchNormalization()(conv1)\n","    conv2 = Conv2D(nFilters, size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(bn1)\n","    bn2 = BatchNormalization()(conv2)\n","    return bn2\n","    \n","def get_model():    \n","    input_layer = Input(shape=(240,240,3))\n","    block1 = get_conv_block(input_layer,32,3)\n","    mp1 = MaxPooling2D(pool_size=(2, 2))(block1)\n","    dr1 = Dropout(0.1)(mp1)\n","    \n","    block2 = get_conv_block(dr1,64,3)\n","    mp2 = MaxPooling2D(pool_size=(2, 2))(block2)\n","    dr2 = Dropout(0.1)(mp2)\n","    \n","    block3 = get_conv_block(dr2,128,3)\n","    mp3 = MaxPooling2D(pool_size=(2, 2))(block3)\n","    dr3 = Dropout(0.1)(mp3)\n","    \n","    block4 = get_conv_block(dr3,256,3)\n","    mp4 = MaxPooling2D(pool_size=(2, 2))(block4)\n","    dr4 = Dropout(0.1)(mp4)\n","\n","    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(dr4)\n","    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n","\n","    up1 = Conv2DTranspose(256,(3,3),strides =(2,2),activation='relu',padding='same',kernel_initializer = 'he_normal')(conv5)\n","    cat1 = concatenate([block4,up1])\n","    dr1 = Dropout(0.1)(cat1)\n","    block5 = get_conv_block(dr1,256,3)\n","\n","    up2 = Conv2DTranspose(128,(3,3),strides =(2,2),activation='relu',padding='same',kernel_initializer = 'he_normal')(block5)\n","    cat2 = concatenate([block3,up2])\n","    dr2 = Dropout(0.1)(cat2)\n","    block6 = get_conv_block(dr2,128,3)\n","    \n","    up3 = Conv2DTranspose(64,(3,3),strides =(2,2),activation='relu',padding='same',kernel_initializer = 'he_normal')(block6)\n","    cat3 = concatenate([block2,up3])\n","    dr3 = Dropout(0.1)(cat3)\n","    block7 = get_conv_block(dr3,64,3)\n","    \n","    up4 = Conv2DTranspose(32,(3,3),strides =(2,2),activation='relu',padding='same',kernel_initializer = 'he_normal')(block7)\n","    cat4 = concatenate([block1,up4])\n","    dr4 = Dropout(0.1)(cat4)\n","    block8 = get_conv_block(dr4,64,3)\n","\n","    conv10 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(block8)\n","    conv11 = Conv2D(4,(1,1), activation='softmax', padding = 'same')(conv10)\n","\n","    model = Model(input_layer,conv11)\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cOY-2Ncp7N1x","colab_type":"code","outputId":"8e65b461-9acc-446d-e386-7368ad7fc14b","executionInfo":{"status":"ok","timestamp":1586236867384,"user_tz":-330,"elapsed":211636,"user":{"displayName":"Ramya B","photoUrl":"","userId":"10194325446164447523"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model = get_model()\n","model.summary()"],"execution_count":11,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, 240, 240, 3)  0                                            \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 240, 240, 32) 896         input_1[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 240, 240, 32) 128         conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 240, 240, 32) 9248        batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 240, 240, 32) 128         conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling2d_1 (MaxPooling2D)  (None, 120, 120, 32) 0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 120, 120, 32) 0           max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 120, 120, 64) 18496       dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 120, 120, 64) 256         conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 120, 120, 64) 36928       batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 120, 120, 64) 256         conv2d_4[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling2d_2 (MaxPooling2D)  (None, 60, 60, 64)   0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 60, 60, 64)   0           max_pooling2d_2[0][0]            \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 60, 60, 128)  73856       dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 60, 60, 128)  512         conv2d_5[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_6 (Conv2D)               (None, 60, 60, 128)  147584      batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 60, 60, 128)  512         conv2d_6[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling2d_3 (MaxPooling2D)  (None, 30, 30, 128)  0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 30, 30, 128)  0           max_pooling2d_3[0][0]            \n","__________________________________________________________________________________________________\n","conv2d_7 (Conv2D)               (None, 30, 30, 256)  295168      dropout_3[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 30, 30, 256)  1024        conv2d_7[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_8 (Conv2D)               (None, 30, 30, 256)  590080      batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 30, 30, 256)  1024        conv2d_8[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling2d_4 (MaxPooling2D)  (None, 15, 15, 256)  0           batch_normalization_8[0][0]      \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 15, 15, 256)  0           max_pooling2d_4[0][0]            \n","__________________________________________________________________________________________________\n","conv2d_9 (Conv2D)               (None, 15, 15, 512)  1180160     dropout_4[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 15, 15, 512)  2359808     conv2d_9[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_transpose_1 (Conv2DTrans (None, 30, 30, 256)  1179904     conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 30, 30, 512)  0           batch_normalization_8[0][0]      \n","                                                                 conv2d_transpose_1[0][0]         \n","__________________________________________________________________________________________________\n","dropout_5 (Dropout)             (None, 30, 30, 512)  0           concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_11 (Conv2D)              (None, 30, 30, 256)  1179904     dropout_5[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_9 (BatchNor (None, 30, 30, 256)  1024        conv2d_11[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_12 (Conv2D)              (None, 30, 30, 256)  590080      batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_10 (BatchNo (None, 30, 30, 256)  1024        conv2d_12[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_transpose_2 (Conv2DTrans (None, 60, 60, 128)  295040      batch_normalization_10[0][0]     \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, 60, 60, 256)  0           batch_normalization_6[0][0]      \n","                                                                 conv2d_transpose_2[0][0]         \n","__________________________________________________________________________________________________\n","dropout_6 (Dropout)             (None, 60, 60, 256)  0           concatenate_2[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_13 (Conv2D)              (None, 60, 60, 128)  295040      dropout_6[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_11 (BatchNo (None, 60, 60, 128)  512         conv2d_13[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_14 (Conv2D)              (None, 60, 60, 128)  147584      batch_normalization_11[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_12 (BatchNo (None, 60, 60, 128)  512         conv2d_14[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_transpose_3 (Conv2DTrans (None, 120, 120, 64) 73792       batch_normalization_12[0][0]     \n","__________________________________________________________________________________________________\n","concatenate_3 (Concatenate)     (None, 120, 120, 128 0           batch_normalization_4[0][0]      \n","                                                                 conv2d_transpose_3[0][0]         \n","__________________________________________________________________________________________________\n","dropout_7 (Dropout)             (None, 120, 120, 128 0           concatenate_3[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_15 (Conv2D)              (None, 120, 120, 64) 73792       dropout_7[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_13 (BatchNo (None, 120, 120, 64) 256         conv2d_15[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_16 (Conv2D)              (None, 120, 120, 64) 36928       batch_normalization_13[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_14 (BatchNo (None, 120, 120, 64) 256         conv2d_16[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_transpose_4 (Conv2DTrans (None, 240, 240, 32) 18464       batch_normalization_14[0][0]     \n","__________________________________________________________________________________________________\n","concatenate_4 (Concatenate)     (None, 240, 240, 64) 0           batch_normalization_2[0][0]      \n","                                                                 conv2d_transpose_4[0][0]         \n","__________________________________________________________________________________________________\n","dropout_8 (Dropout)             (None, 240, 240, 64) 0           concatenate_4[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_17 (Conv2D)              (None, 240, 240, 64) 36928       dropout_8[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_15 (BatchNo (None, 240, 240, 64) 256         conv2d_17[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_18 (Conv2D)              (None, 240, 240, 64) 36928       batch_normalization_15[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_16 (BatchNo (None, 240, 240, 64) 256         conv2d_18[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_19 (Conv2D)              (None, 240, 240, 2)  1154        batch_normalization_16[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_20 (Conv2D)              (None, 240, 240, 4)  12          conv2d_19[0][0]                  \n","==================================================================================================\n","Total params: 8,685,710\n","Trainable params: 8,681,742\n","Non-trainable params: 3,968\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YDodKs-AMB81","colab_type":"code","colab":{}},"source":["def dice_coef(y_true, y_pred):\n","    y_true_f = K.flatten(y_true)\n","    y_pred_f = K.flatten(y_pred)\n","    intersection = K.sum(y_true_f * y_pred_f)\n","    return (2. * intersection + 1) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oMnXb4wMMB88","colab_type":"code","colab":{}},"source":["import math\n","from keras.callbacks import EarlyStopping,ModelCheckpoint\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n","mc = ModelCheckpoint('Checkpoint.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CyEYstIcMB9B","colab_type":"code","colab":{}},"source":["optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TH9yDcoO6Kyg","colab_type":"code","colab":{}},"source":["def tversky_loss(y_true, y_pred):\n","    alpha = 0.5\n","    beta  = 0.5\n","    \n","    ones = K.ones(K.shape(y_true))\n","    p0 = y_pred      # proba that voxels are class i\n","    p1 = ones-y_pred # proba that voxels are not class i\n","    g0 = y_true\n","    g1 = ones-y_true\n","    \n","    num = K.sum(p0*g0, (0,1,2))\n","    den = num + alpha*K.sum(p0*g1,(0,1,2)) + beta*K.sum(p1*g0,(0,1,2))\n","    \n","    T = K.sum(num/den) # when summing over classes, T has dynamic range [0 Ncl]\n","    \n","    Ncl = K.cast(K.shape(y_true)[-1], 'float32')\n","    return Ncl-T\n","\n","def combined_loss(y_true, y_pred):\n","  return (1*K.categorical_crossentropy(y_true, y_pred))+(0.5*tversky_loss(y_true, y_pred))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yIzmCHVMMB9I","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"74122e38-02d0-4adf-dfe0-29ddb1691e3d","executionInfo":{"status":"ok","timestamp":1586236867390,"user_tz":-330,"elapsed":209271,"user":{"displayName":"Ramya B","photoUrl":"","userId":"10194325446164447523"}}},"source":["from keras import backend as K      \n","model.compile(loss=combined_loss, optimizer=optimizer, metrics=[dice_coef,'accuracy'])"],"execution_count":16,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cW1V8Wi0ZErc","colab_type":"code","outputId":"bfced1e4-543f-4650-d0bf-64beb3f14452","executionInfo":{"status":"error","timestamp":1586237705748,"user_tz":-330,"elapsed":1047227,"user":{"displayName":"Ramya B","photoUrl":"","userId":"10194325446164447523"}},"colab":{"base_uri":"https://localhost:8080/","height":480}},"source":["num_epoch = 100\n","MODELS_PATH = \"/content/drive/My Drive/Breast Cancer Treatment/Models/4 Class/ER\"\n","history = model.fit(TrainX, TrainY, epochs=num_epoch, validation_data=(ValidX, ValidY), shuffle=True,batch_size=16,callbacks=[es,mc])\n","model.save(MODELS_PATH + '/4 Class_UNET_ER_07_04_2020.h5')"],"execution_count":17,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Train on 1800 samples, validate on 120 samples\n","Epoch 1/100\n"," 576/1800 [========>.....................] - ETA: 28:43 - loss: 1.6196 - dice_coef: 0.8151 - acc: 0.8626"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-b2248c336b35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mMODELS_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/My Drive/Breast Cancer Treatment/Models/4 Class/ER\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mValidX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValidY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODELS_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/4 Class_UNET_ER_07_04_2020.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"Ueu1X5gBMB9e","colab_type":"code","cellView":"both","colab":{}},"source":["train_loss = history.history['loss']\n","train_acc = history.history['acc']\n","val_loss = history.history['val_loss']\n","val_acc = history.history['val_acc']\n","xc=range(len(train_loss))\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","plt.plot(xc, train_loss)\n","plt.plot(xc, val_loss)\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend(['train','val'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1DvY6AotMB-T","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","plt.plot(xc, train_acc)\n","plt.plot(xc, val_acc)\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Acc\")\n","plt.legend(['train','val'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hr402gEaf6Ix","colab_type":"code","colab":{}},"source":["# MODELS_PATH = \"/content/drive/My Drive/Breast Cancer Treatment/Models\"\n","# model = keras.models.load_model(MODELS_PATH+'/UNET_ER_20_03_03_v5.h5',custom_objects={ 'combined_loss': combined_loss, 'dice_coef': dice_coef })"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LAKqsYxXMB-0","colab_type":"code","colab":{}},"source":["# Function to find the most probable class of an image pixel by pixel\n","def ClassFinder(img):\n","  result = np.zeros(img.shape, dtype=np.uint8)\n","  for r in range(len(img)):\n","    for c in range(len(img[0])):\n","      idx = np.argmax(img[r][c])\n","      result[r][c][idx]=1\n","      \n","  return result    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"C7ud1f_mMB_C","colab_type":"code","colab":{}},"source":["train_result = model.predict(TrainX, batch_size=16)\n","print(np.shape(train_result))\n","Train_predict = [ ]\n","for img in train_result:\n","  img = ClassFinder(img)\n","  encoded_img = convertFromOneHot(img)\n","  Train_predict.append(encoded_img)\n","\n","Predicted_images = np.asarray(Train_predict)\n","print(Predicted_images.shape)\n","np.unique(Predicted_images)\n","# dice = dc(train_result, TrainY)\n","# pre  = precision(train_result,TrainY)\n","# re   = recall(train_result,TrainY)\n","# print('Train dc: '  + str(dice))\n","# print('Train pre: ' + str(pre))\n","# print('Train re: '  + str(re))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"40g1oWTtQElx","colab_type":"code","colab":{}},"source":["test_result = model.predict(TestX, batch_size=16)\n","print(np.shape(test_result))\n","\n","Test_predict = [ ]\n","for img in test_result:\n","  img = ClassFinder(img)\n","  encoded_img = convertFromOneHot(img)\n","  Test_predict.append(encoded_img)\n","\n","Predicted_images_test = np.asarray(Test_predict)\n","print(Predicted_images_test.shape)\n","np.unique(Predicted_images_test)\n","# dice = dc(test_result, TestY)\n","# pre  = precision(test_result,TestY)\n","# re   = recall(test_result,TestY)\n","# print('Test dc: '  + str(dice))\n","# print('Test pre: ' + str(pre))\n","# print('Test re: '  + str(re))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cS7lN30b9oAX","colab_type":"code","colab":{}},"source":["from sklearn.metrics import confusion_matrix\n","def get_confusion_matrix(Actual, Predicted):\n","  ActualY = np.reshape(Actual,(-1, Actual.shape[3]))\n","  PredY = np.reshape(Predicted,(-1, Predicted.shape[3]))\n","  matrix = confusion_matrix(ActualY.argmax(axis = 1), PredY.argmax(axis = 1))\n","  return matrix\n","\n","def get_precision(confusion_matrix):\n","  precision = np.diag(confusion_matrix) / np.sum(confusion_matrix, axis = 0)\n","  nan_indices = np.isnan(precision)\n","  precision[nan_indices] = 0\n","  return precision\n","\n","def get_recall(confusion_matrix):\n","  recall = np.diag(confusion_matrix) / np.sum(confusion_matrix, axis = 1)\n","  nan_indices = np.isnan(recall)\n","  recall[nan_indices] = 0\n","  return recall\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZF2dV0k5-JME","colab_type":"code","colab":{}},"source":["test_matrix = get_confusion_matrix(TestY, test_result)\n","train_matrix = get_confusion_matrix(TrainY, train_result)\n","print(test_matrix)\n","print(train_matrix)\n","# TestY.shape[3]\n","# test_result.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qwmK9wnRueRb","colab_type":"code","colab":{}},"source":["np.unique(Predicted_images_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VYeWreCuAD4Y","colab_type":"code","colab":{}},"source":["test_precision = get_precision(test_matrix)\n","print('Test Precision')\n","print(test_precision)\n","test_recall = get_recall(test_matrix)\n","print('Test Recall')\n","print(test_recall)\n","train_precision = get_precision(train_matrix)\n","print('Train Precision')\n","print(train_precision)\n","train_recall = get_recall(train_matrix)\n","print('Train Recall')\n","print(train_recall)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bOKLli0hBcGm","colab_type":"code","colab":{}},"source":["# weight_matrix = np.zeros((3,1))\n","# weight_matrix[0] = 0.6\n","# weight_matrix[1] = 0.2\n","# weight_matrix[2] = 0.2\n","# print(weight_matrix.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jC2VHapw-bMW","colab_type":"code","colab":{}},"source":["weight_matrix = np.zeros((3,1))\n","weight_matrix[0] = 1/3\n","weight_matrix[1] = 1/3\n","weight_matrix[2] = 1/3\n","print(weight_matrix.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5PO1RA0vCHBR","colab_type":"code","colab":{}},"source":["def get_overall_precision(precision, weights):\n","  final_precision = np.dot(precision,weights)\n","  return final_precision\n","\n","def get_overall_recall(recall, weights):\n","  final_recall = np.dot(recall ,weights)\n","  return final_recall"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VjZqxOX8EzbP","colab_type":"code","colab":{}},"source":["final_test_precision  = get_overall_precision(test_precision, weight_matrix)\n","final_test_recall     = get_overall_recall(test_recall, weight_matrix)\n","final_train_precision = get_overall_precision(train_precision, weight_matrix)\n","final_train_recall    = get_overall_recall(train_recall, weight_matrix)\n","print('Final Test Precision')\n","print(final_test_precision)\n","print('Final Test Recall')\n","print(final_test_recall)\n","print('Final Train Precision')\n","print(final_train_precision)\n","print('Final Train Recall')\n","print(final_train_recall)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JiGEcKllGbC6","colab_type":"code","colab":{}},"source":["from scipy.stats import hmean\n","\n","def dice_coefficient(precision, recall):\n","  return hmean(np.asarray([precision , recall]))\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pGWVEIDgHD4I","colab_type":"code","colab":{}},"source":["test_dice_coeff = dice_coefficient(final_test_precision, final_test_recall)\n","print('Test Dice coeff')\n","print(test_dice_coeff)\n","\n","train_dice_coeff = dice_coefficient(final_train_precision, final_train_recall)\n","print('Train Dice coeff')\n","print(train_dice_coeff)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W4ZaoO0mMoX6","colab_type":"code","colab":{}},"source":["copy1  = np.copy(TestX [250])\n","copy2 = copy1.astype('float32')*255\n","copy2 = copy2.astype('uint8')\n","copy3 = np.reshape(copy2,(240, 240,3))\n","Img = cv2.cvtColor(copy3,cv2.COLOR_BGR2RGB)\n","\n","Mask = TestGroundTruth[250]\n","\n","Predicted_Mask = Predicted_images_test[250] \n","\n","plt.subplot(131).imshow(Img)\n","plt.subplot(132).imshow(Mask,'gray')\n","plt.subplot(133).imshow(Predicted_Mask,'gray')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XzwipgD8UuIy","colab_type":"code","colab":{}},"source":["def stitchMaskPatches(pieces):\n","  k = 0\n","  reconstructed_img = np.ones([1440,1920])\n","  for r in range(6):\n","    row = r * 240\n","    for c in range(8):\n","      col = c * 240\n","      reconstructed_img[row:row+240,col:col+240] = pieces[k]\n","      k = k + 1\n","  return reconstructed_img\n","\n","\n","def stitchImagePatches(pieces):\n","  k = 0\n","  reconstructed_img = np.ones([1440,1920,3])\n","  for r in range(6):\n","    row = r * 240\n","    for c in range(8):\n","      col = c * 240\n","      reconstructed_img[row:row+240,col:col+240,:] = pieces[k]\n","      k = k + 1\n","  return reconstructed_img\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i7rOZOq1M7lm","colab_type":"code","colab":{}},"source":["def saveNumpyOutput(mask):\n","  num = mask.shape[0]/480\n","  for id in range(int(num)):\n","    temp = []\n","    for i in range(id, id+10):\n","      idx = i * 48\n","      final_output = mask[idx:idx+48]\n","      final_output = stitchMaskPatches(final_output)\n","      temp.append(final_output)\n","    final_output = np.asarray(temp)\n","    np.save(\"/content/drive/My Drive/Breast Cancer Treatment/Numpy Arrays/Predicted Output/Combined_Loss/ER1 Patient \" + str(id), final_output)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W_B0gb-OQS53","colab_type":"code","colab":{}},"source":["# saving predicted outputs as numpy arrays for test images\n","saveNumpyOutput(Predicted_images)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DzUOdXOKU2nb","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import cv2\n","import numpy as np\n","from PIL import Image\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from google.colab.patches import cv2_imshow\n","\n","\n","plt.figure(figsize=(20,20))\n","id = 25   # enter between 0- 40 since there are 4 patients with 10 images each\n","\n","id = id * 48\n","final_input = TestX[id:id+48]\n","\n","Mask_input = TestY[id:id+48]\n","Test_Actual = [ ]\n","for img in Mask_input:\n","  img = ClassFinder(img)\n","  encoded_img = convertFromOneHot(img)\n","  Test_Actual.append(encoded_img)\n","  \n","print(Mask_input.shape)\n","Mask_input = np.asarray(Test_Actual)\n","print(Mask_input.shape)\n","\n","final_output = Predicted_images_test[id:id+48]\n","\n","final_input = stitchImagePatches(final_input)\n","Mask_input =  stitchMaskPatches(Mask_input)\n","final_output = stitchMaskPatches(final_output)\n","\n","print(final_input.shape)\n","print(final_output.shape)\n","print(Mask_input.shape)\n","\n","print(np.unique(final_output))\n","copy1  = np.copy(final_input)\n","copy2 = copy1.astype('float32')*255\n","copy2 = copy2.astype('uint8')\n","final_input = np.reshape(copy2,(1440, 1920,3))\n","final_input = cv2.cvtColor(final_input,cv2.COLOR_BGR2RGB)\n","\n","plt.subplot(131).imshow(final_input)\n","plt.subplot(132).imshow(Mask_input,'gray')\n","plt.subplot(133).imshow(final_output,'gray')\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t5IqVx01JeWn","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import cv2\n","import numpy as np\n","from PIL import Image\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from google.colab.patches import cv2_imshow\n","\n","\n","plt.figure(figsize=(20,20))\n","id = 14  # enter between 0- 40 since there are 4 patients with 10 images each\n","\n","id = id * 48\n","final_input = TrainX[id:id+48]\n","\n","Mask_input = TrainY[id:id+48]\n","Train_Actual = [ ]\n","for img in Mask_input:\n","  img = ClassFinder(img)\n","  encoded_img = convertFromOneHot(img)\n","  Train_Actual.append(encoded_img)\n","  \n","print(Mask_input.shape)\n","Mask_input = np.asarray(Train_Actual)\n","print(Mask_input.shape)\n","\n","final_output = Predicted_images[id:id+48]\n","\n","final_input = stitchImagePatches(final_input)\n","Mask_input =  stitchMaskPatches(Mask_input)\n","final_output = stitchMaskPatches(final_output)\n","\n","print(final_input.shape)\n","print(final_output.shape)\n","print(Mask_input.shape)\n","\n","print(np.unique(final_output))\n","copy1  = np.copy(final_input)\n","copy2 = copy1.astype('float32')*255\n","copy2 = copy2.astype('uint8')\n","final_input = np.reshape(copy2,(1440, 1920,3))\n","final_input = cv2.cvtColor(final_input,cv2.COLOR_BGR2RGB)\n","\n","plt.subplot(131).imshow(final_input)\n","plt.subplot(132).imshow(Mask_input,'gray')\n","plt.subplot(133).imshow(final_output,'gray')\n"],"execution_count":0,"outputs":[]}]}