{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HER2NET.ipynb","provenance":[{"file_id":"16hqggEiNT-NR-gwzWl-JDg6OSUab1SSE","timestamp":1590932441130},{"file_id":"1dLxM_AMOSs32Mxu42FNwumVVm11sLpeG","timestamp":1589304160359},{"file_id":"17lvwlvqQV7B-wqN2ajx7Q2UwCZwCd-WD","timestamp":1586698406636},{"file_id":"1SAeOlks1aRTndKfvFcMtNB7V3Ik6nfN0","timestamp":1586361815646},{"file_id":"1puNfvA8GEtqbsfDMP30_RS6XRrNM8Ky8","timestamp":1586249758540},{"file_id":"1QPxGukBl9nFKfyxsNhM4lUw4rYauJaY1","timestamp":1586242198727},{"file_id":"1txy9g7gYnELmPTY6sT4JtfbhwQthIyoG","timestamp":1583812009231},{"file_id":"1a9BwVwMfQ4iFdte5xgs4kzj58JNqs4Vu","timestamp":1579539718864},{"file_id":"1KyLESiefS0uSAi_kTOAZ9NmNUSYTs9lL","timestamp":1578995412588},{"file_id":"1YDTADQrLw8-aYCSjOEYxDuIB1nuy3Xzm","timestamp":1577686452735},{"file_id":"1DqrMpMvIbzPtWFjsL_eNwrVM-xaFvx6c","timestamp":1571727299009}],"collapsed_sections":["ZwjDP44MUTa6"],"machine_shape":"hm"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ixvBWa23MB7e","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1595563758756,"user_tz":-330,"elapsed":21771,"user":{"displayName":"Bhavishya Viswanath","photoUrl":"","userId":"05271986293024817587"}},"outputId":"e90a9f46-b929-4d1d-b6d7-efed660611a6"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LzNbXYWE4I5E","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595563760676,"user_tz":-330,"elapsed":1896,"user":{"displayName":"Bhavishya Viswanath","photoUrl":"","userId":"05271986293024817587"}}},"source":["import os\n","import cv2\n","import numpy as np"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Mk6pnB6tFp0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595563762917,"user_tz":-330,"elapsed":3987,"user":{"displayName":"Bhavishya Viswanath","photoUrl":"","userId":"05271986293024817587"}},"outputId":"192faa54-4241-4965-8af6-ad8416a1d69f"},"source":["import math\n","import random\n","import keras\n","from keras.layers import *\n","from keras.models import Sequential\n","from keras import Model\n","from keras import backend as K  \n","from keras.regularizers import l2\n","from keras.callbacks import EarlyStopping,ModelCheckpoint\n","import tensorflow as tf\n","from sklearn import metrics"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"U4SPHrcLZH4a","colab":{},"executionInfo":{"status":"ok","timestamp":1595563762919,"user_tz":-330,"elapsed":3089,"user":{"displayName":"Bhavishya Viswanath","photoUrl":"","userId":"05271986293024817587"}}},"source":["def get_conv_block(input_layer,nFilters,size):\n","    conv1 = Conv2D(nFilters, size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',kernel_regularizer=l2(0.046))(input_layer)\n","    conv2 = Conv2D(nFilters, size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',kernel_regularizer=l2(0.046))(conv1)\n","    return conv2\n","\n","def get_conv_block1(input_layer,nFilters,size):\n","    conv1 = Conv2D(nFilters, size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',kernel_regularizer=l2(0.046))(input_layer)\n","    conv2 = Conv2D(nFilters, size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',kernel_regularizer=l2(0.046))(conv1)\n","    conv3 = Conv2D(nFilters, size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',kernel_regularizer=l2(0.046))(conv2)\n","    return conv3\n","\n","def get_deconv_block(input_layer,nFilters,size):\n","    conv1 = Conv2DTranspose(nFilters, size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',kernel_regularizer=l2(0.046))(input_layer)\n","    conv2 = Conv2DTranspose(nFilters, size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',kernel_regularizer=l2(0.046))(conv1)\n","    return conv2\n","\n","def get_deconv_block1(input_layer,nFilters,size):\n","    conv1 = Conv2DTranspose(nFilters, size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',kernel_regularizer=l2(0.046))(input_layer)\n","    conv2 = Conv2DTranspose(nFilters, size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',kernel_regularizer=l2(0.046))(conv1)\n","    conv3 = Conv2DTranspose(nFilters, size, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',kernel_regularizer=l2(0.046))(conv2)\n","    return conv3\n","    \n","def get_HER2NET(input_layer,nFilters): \n","\n","    block1 = get_conv_block(input_layer,nFilters,3)\n","    mp1 = MaxPooling2D(pool_size=(2, 2))(block1)\n","    dr1 = Dropout(0.1)(mp1)\n","   \n","    block2_1 = get_conv_block(dr1,nFilters,3)\n","    block2_2 = get_conv_block(dr1,nFilters,5)\n","    cat2 = concatenate([block2_1, block2_2])\n","    mp2 = MaxPooling2D(pool_size=(2, 2))(cat2)\n","    dr2 = Dropout(0.1)(mp2)\n"," \n","    block3_1 = get_conv_block(dr2,nFilters,3)\n","    block3_2 = get_conv_block(dr2,nFilters,5)\n","    cat3 = concatenate([block3_1, block3_2])\n","    mp3 = MaxPooling2D(pool_size=(2, 2))(cat3)\n","    dr3 = Dropout(0.1)(mp3)\n","\n","    mp4 = MaxPooling2D(pool_size=(2, 2))(dr3)\n","    flat = Reshape((mp4.shape[1],mp4.shape[2]*mp4.shape[3]))(mp4)\n","    lstm = LSTM(mp4.shape[1], return_sequences=True, return_state=True)\n","    lstm1, final_memory_state, final_carry_state = lstm(flat)\n","    lstm1 = Reshape((mp4.shape[1],mp4.shape[1],1))(lstm1)\n","    print(lstm1.shape)\n","\n","    block4_1 = get_conv_block1(mp4,nFilters,3)\n","    block4_2 = get_conv_block1(mp4,nFilters,5)\n","    cat4 = concatenate([block4_1, block4_2])\n","\n","    conv_out = concatenate([cat4,lstm1])\n","\n","    up1 = UpSampling2D(size=(2,2))(conv_out)\n","    \n","    flat = Reshape((up1.shape[1],up1.shape[2]*up1.shape[3]))(up1)\n","    lstm = LSTM(up1.shape[1], return_sequences=True, return_state=True)\n","    lstm2, final_memory_state, final_carry_state = lstm(flat)\n","    lstm2 = Reshape((up1.shape[1],up1.shape[1],1))(lstm2)\n","    print(lstm2.shape)\n","\n","    block4_1 = get_deconv_block1(up1,nFilters,3)\n","    block4_2 = get_deconv_block1(up1,nFilters,5)\n","    cat4 = concatenate([block4_1, block4_2])\n","    dr4 = Dropout(0.1)(cat4)\n","\n","    LSTM_concat = concatenate([dr4,lstm2])\n","    \n","    up2 = UpSampling2D(size=(2,2))(LSTM_concat)\n","\n","    block3_1 = get_deconv_block(up2,nFilters,3)\n","    block3_2 = get_deconv_block(up2,nFilters,5)\n","    cat3 = concatenate([block3_1, block3_2])\n","    dr3 = Dropout(0.1)(cat3)\n","    up3 = UpSampling2D(size=(2,2))(dr3)\n","\n","    block2_1 = get_deconv_block(up3,nFilters,3)\n","    block2_2 = get_deconv_block(up3,nFilters,5)\n","    cat2 = concatenate([block2_1, block2_2])\n","    dr2 = Dropout(0.1)(cat2)\n","    up2 = UpSampling2D(size=(2,2))(dr2)\n","\n","    block1 = get_deconv_block(up2,nFilters,3)\n","    dr1 = Dropout(0.1)(block1)\n","    up1 = UpSampling2D(size=(2,2))(dr1)\n","\n","    dense = Dense(32, activation = 'relu', kernel_initializer = 'he_normal',kernel_regularizer=l2(0.046))(dr1)\n","    out = Dense(3, activation = 'softmax', kernel_initializer = 'he_normal',kernel_regularizer=l2(0.046))(dense)\n","\n","    return out\n","\n","def get_model(input_shape,nFilters):\n","\n","    input_layer = Input(shape=input_shape)\n","    output_layer = get_HER2NET(input_layer,nFilters)\n","    model = Model(input_layer,output_layer)\n","    return model"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"xiXrY-SjvEMW","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595563808600,"user_tz":-330,"elapsed":47721,"user":{"displayName":"Bhavishya Viswanath","photoUrl":"","userId":"05271986293024817587"}}},"source":["PATH = \"/content/drive/My Drive/Breast Cancer Treatment/Numpy Arrays/HER2/\"\n","\n","# Loading the data patient wise\n","X1 = np.load(PATH + 'Sliced Images/IHC 221.npy')\n","Y1 = np.load(PATH + 'Sliced Ground Truth/IHC 221.npy')\n","\n","X2 = np.load(PATH + 'Sliced Images/IHC 229.npy')\n","Y2 = np.load(PATH + 'Sliced Ground Truth/IHC 229.npy')\n","\n","X3 = np.load(PATH + 'Sliced Images/IHC 232.npy')\n","Y3 = np.load(PATH + 'Sliced Ground Truth/IHC 232.npy')\n","\n","X4 = np.load(PATH + 'Sliced Images/IHC 239.npy')\n","Y4 = np.load(PATH + 'Sliced Ground Truth/IHC 239.npy')\n","\n","X5 = np.load(PATH + 'Sliced Images/IHC 242.npy')\n","Y5 = np.load(PATH + 'Sliced Ground Truth/IHC 242.npy')\n","\n","X6 = np.load(PATH + 'Sliced Images/IHC 246.npy')\n","Y6 = np.load(PATH + 'Sliced Ground Truth/IHC 246.npy')\n","\n","X7 = np.load(PATH + 'Sliced Images/IHC 252.npy')\n","Y7 = np.load(PATH + 'Sliced Ground Truth/IHC 252.npy')\n","\n","X8 = np.load(PATH + 'Sliced Images/IHC 254.npy')\n","Y8 = np.load(PATH + 'Sliced Ground Truth/IHC 254.npy')\n","\n","X9 = np.load(PATH + 'Sliced Images/IHC 263.npy')\n","Y9 = np.load(PATH + 'Sliced Ground Truth/IHC 263.npy')"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"hgAEBm703KCC","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595563808603,"user_tz":-330,"elapsed":46074,"user":{"displayName":"Bhavishya Viswanath","photoUrl":"","userId":"05271986293024817587"}}},"source":["X = [X1, X2, X3, X4, X5, X6, X7, X8, X9]\n","Y = [Y1, Y2, Y3, Y4, Y5, Y6, Y7, Y8, Y9]\n","patient_no = ['221','229','232','239','242','246','252','254','263']\n","size = [10,10,10,10,10,10,11,10,10]"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"VWE-QvIXeBrW","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595563808606,"user_tz":-330,"elapsed":45302,"user":{"displayName":"Bhavishya Viswanath","photoUrl":"","userId":"05271986293024817587"}}},"source":["fold_split = []\n","\n","#test patient = 221, 229\n","fold_split.append(([2,3,4,5,6,7,8],[0,1]))\n","\n","#test_patient = 239, 252\n","fold_split.append(([0,1,2,4,5,7,8],[3,6]))\n","\n","#test_patient = 252,263\n","fold_split.append(([0,1,2,3,4,6,8],[5,7]))"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"6MujkbhpsnE9","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595563808607,"user_tz":-330,"elapsed":44514,"user":{"displayName":"Bhavishya Viswanath","photoUrl":"","userId":"05271986293024817587"}}},"source":["def convertToLabels(data):\n","  data[data==128]=1\n","  data[data==255]=2\n","\n","def convertFromLabels(data):\n","  data[data==1]=128\n","  data[data==2]=255"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"YDodKs-AMB81","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595563808609,"user_tz":-330,"elapsed":43546,"user":{"displayName":"Bhavishya Viswanath","photoUrl":"","userId":"05271986293024817587"}}},"source":["def dice_coef(y_true, y_pred):\n","    y_true_f = K.flatten(y_true)\n","    y_pred_f = K.flatten(y_pred)\n","    intersection = K.sum(y_true_f * y_pred_f)\n","    return (2. * intersection + 1) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"TH9yDcoO6Kyg","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595563808610,"user_tz":-330,"elapsed":42825,"user":{"displayName":"Bhavishya Viswanath","photoUrl":"","userId":"05271986293024817587"}}},"source":["def tversky_loss_1(y_true, y_pred):\n","    alpha = 0.7\n","    beta  = 0.3\n","    \n","    ones = K.ones(K.shape(y_true))\n","    p0 = y_pred      # proba that voxels are class i\n","    p1 = ones-y_pred # proba that voxels are not class i\n","  \n","    g0 = y_true\n","    g1 = ones-y_true\n","    \n","    num = K.sum(p0*g0, (0,1,2))\n","    den = num + alpha*K.sum(p0*g1,(0,1,2)) + beta*K.sum(p1*g0,(0,1,2))\n","    T = K.sum(num/den) # when summing over classes, T has dynamic range [0 Ncl]\n","    \n","    Ncl = K.cast(K.shape(y_true)[-1], 'float32')\n","    return Ncl-T\n","\n","def tversky_loss_2(y_true, y_pred):\n","    alpha = 0.3\n","    beta  = 0.7\n","    \n","    ones = K.ones(K.shape(y_true))\n","    p0 = y_pred      # proba that voxels are class i\n","    p1 = ones-y_pred # proba that voxels are not class i\n","    g0 = y_true\n","    g1 = ones-y_true\n","    \n","    num = K.sum(p0*g0, (0,1,2))\n","    den = num + alpha*K.sum(p0*g1,(0,1,2)) + beta*K.sum(p1*g0,(0,1,2))\n","\n","    T = K.sum(num/den) # when summing over classes, T has dynamic range [0 Ncl]\n","    \n","    Ncl = K.cast(K.shape(y_true)[-1], 'float32')\n","    return Ncl-T\n","\n","def focal_tversky_loss_1(y_true, y_pred, gamma=0.75):\n","    tversky_index = tversky_loss_1(y_true, y_pred)\n","    return K.pow((tversky_index), gamma)\n","  \n","def focal_tversky_loss_2(y_true, y_pred, gamma=0.75):\n","    tversky_index = tversky_loss_2(y_true, y_pred)\n","    return K.pow((tversky_index), gamma)\n","\n","def combined_loss(y_true, y_pred):\n","  return (0.2*K.categorical_crossentropy(y_true, y_pred))+(0.8*focal_tversky_loss_1(y_true, y_pred)+(0.8*focal_tversky_loss_2(y_true, y_pred)))"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"SaGvHbqtarli","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595563808611,"user_tz":-330,"elapsed":41716,"user":{"displayName":"Bhavishya Viswanath","photoUrl":"","userId":"05271986293024817587"}}},"source":["batch_size = 8\n","def get_batch(batch_size, X_train, Y_train): \n","    size_batch = batch_size\n","    last_index = len(X_train) - 1\n","    x_train = X_train\n","    y_train = Y_train \n","    while True:\n","        batch_data = [[],[]]\n","        for i in range(0, size_batch):\n","            random_index = random.randint(0, last_index)\n","            batch_data[0].append(x_train[random_index])\n","            batch_data[1].append(y_train[random_index])\n","\n","        yield (np.array(batch_data[0]), np.array(batch_data[1]))     "],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"59LOrWNyeE9X","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"11a38f5c-0cbe-4495-e525-605bdb99c08f"},"source":["MODELS_PATH = \"/content/drive/My Drive/Breast Cancer Treatment/Models/HER2/3Fold HER2NET/\"\n","\n","fold=0\n","\n","for train_index, test_index in fold_split:\n","  print(\"TRAIN:\", train_index, \"TEST:\", test_index, \"FOLD:\", fold)\n","\n","  TrainX =  np.concatenate(np.array([X[i] for i in train_index]))\n","  TrainY =  np.concatenate(np.array([Y[i] for i in train_index]))\n","  TestX  =  np.concatenate(np.array([X[i] for i in test_index]))\n","  TestY  =  np.concatenate(np.array([Y[i] for i in test_index]))\n","\n","  Train = list(zip(TrainX,TrainY))\n","  random.shuffle(Train)\n","  TrainX,TrainY = zip(*Train)\n","  TrainX = np.asarray(list(TrainX))\n","  TrainY = np.asarray(list(TrainY))\n","\n","  Test = list(zip(TestX,TestY))\n","  random.shuffle(Test)\n","  TestX,TestY = zip(*Test)\n","  TestX = np.asarray(list(TestX))\n","  TestY = np.asarray(list(TestY))\n","\n","  TrainX = TrainX.astype('float32')/255\n","  TestX = TestX.astype('float32')/255\n","  convertToLabels(TrainY)\n","  TrainY = keras.utils.to_categorical(TrainY,num_classes=3,dtype='int16')\n","  convertToLabels(TestY)\n","  TestY = keras.utils.to_categorical(TestY,num_classes=3,dtype='int16')\n","  ValX = TrainX[(int)(0.8*TrainX.shape[0]):]\n","  ValY = TrainY[(int)(0.8*TrainY.shape[0]):]\n","  model = get_model((240,240,3),64)\n","\n","  mc = ModelCheckpoint('Checkpoint.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n","  optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.5)\n","\n","  model.compile(loss=combined_loss, optimizer= optimizer , metrics=[dice_coef,'accuracy']) \n","  num_epoch = 100\n","  datagen = get_batch(batch_size, TrainX, TrainY)\n","  n_points = len(TrainX)\n","  print('-----------fold {}--------------'.format(fold))\n","  history = model.fit(datagen, validation_data = [ValX, ValY],\n","                  epochs=num_epoch,steps_per_epoch = math.ceil(n_points / batch_size), callbacks =[mc],  shuffle =True)\n","  model.save(MODELS_PATH + '/HER2NET_'+ str(fold) +'.h5')\n","  fold = fold + 1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TRAIN: [2, 3, 4, 5, 6, 7, 8] TEST: [0, 1] FOLD: 0\n","(None, 15, 15, 1)\n","(None, 30, 30, 1)\n","-----------fold 0--------------\n","Epoch 1/100\n","426/426 [==============================] - 108s 253ms/step - loss: 121.1960 - dice_coef: 0.4569 - accuracy: 0.6161 - val_loss: 51.5888 - val_dice_coef: 0.4425 - val_accuracy: 0.6468\n","\n","Epoch 00001: val_loss improved from inf to 51.58876, saving model to Checkpoint.h5\n","Epoch 2/100\n","426/426 [==============================] - 97s 228ms/step - loss: 27.4797 - dice_coef: 0.4633 - accuracy: 0.6478 - val_loss: 13.0021 - val_dice_coef: 0.4743 - val_accuracy: 0.6468\n","\n","Epoch 00002: val_loss improved from 51.58876 to 13.00209, saving model to Checkpoint.h5\n","Epoch 3/100\n","426/426 [==============================] - 97s 227ms/step - loss: 7.9917 - dice_coef: 0.4834 - accuracy: 0.6556 - val_loss: 4.9897 - val_dice_coef: 0.4801 - val_accuracy: 0.6468\n","\n","Epoch 00003: val_loss improved from 13.00209 to 4.98966, saving model to Checkpoint.h5\n","Epoch 4/100\n","426/426 [==============================] - 97s 227ms/step - loss: 3.9466 - dice_coef: 0.4862 - accuracy: 0.6541 - val_loss: 3.3255 - val_dice_coef: 0.4832 - val_accuracy: 0.6468\n","\n","Epoch 00004: val_loss improved from 4.98966 to 3.32553, saving model to Checkpoint.h5\n","Epoch 5/100\n","426/426 [==============================] - 97s 229ms/step - loss: 3.1054 - dice_coef: 0.4876 - accuracy: 0.6537 - val_loss: 2.9800 - val_dice_coef: 0.4862 - val_accuracy: 0.6468\n","\n","Epoch 00005: val_loss improved from 3.32553 to 2.97998, saving model to Checkpoint.h5\n","Epoch 6/100\n","426/426 [==============================] - 97s 227ms/step - loss: 2.9337 - dice_coef: 0.4874 - accuracy: 0.6516 - val_loss: 2.9081 - val_dice_coef: 0.4874 - val_accuracy: 0.6468\n","\n","Epoch 00006: val_loss improved from 2.97998 to 2.90812, saving model to Checkpoint.h5\n","Epoch 7/100\n","426/426 [==============================] - 96s 226ms/step - loss: 2.8971 - dice_coef: 0.4865 - accuracy: 0.6498 - val_loss: 2.8932 - val_dice_coef: 0.4873 - val_accuracy: 0.6468\n","\n","Epoch 00007: val_loss improved from 2.90812 to 2.89320, saving model to Checkpoint.h5\n","Epoch 8/100\n","426/426 [==============================] - 96s 226ms/step - loss: 2.8868 - dice_coef: 0.4880 - accuracy: 0.6500 - val_loss: 2.8901 - val_dice_coef: 0.4846 - val_accuracy: 0.6468\n","\n","Epoch 00008: val_loss improved from 2.89320 to 2.89006, saving model to Checkpoint.h5\n","Epoch 9/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8866 - dice_coef: 0.4867 - accuracy: 0.6504 - val_loss: 2.8895 - val_dice_coef: 0.4866 - val_accuracy: 0.6468\n","\n","Epoch 00009: val_loss improved from 2.89006 to 2.88947, saving model to Checkpoint.h5\n","Epoch 10/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8846 - dice_coef: 0.4914 - accuracy: 0.6547 - val_loss: 2.8894 - val_dice_coef: 0.4876 - val_accuracy: 0.6468\n","\n","Epoch 00010: val_loss improved from 2.88947 to 2.88936, saving model to Checkpoint.h5\n","Epoch 11/100\n","426/426 [==============================] - 96s 226ms/step - loss: 2.8857 - dice_coef: 0.4889 - accuracy: 0.6516 - val_loss: 2.8893 - val_dice_coef: 0.4872 - val_accuracy: 0.6468\n","\n","Epoch 00011: val_loss improved from 2.88936 to 2.88928, saving model to Checkpoint.h5\n","Epoch 12/100\n","426/426 [==============================] - 96s 226ms/step - loss: 2.8863 - dice_coef: 0.4915 - accuracy: 0.6543 - val_loss: 2.8893 - val_dice_coef: 0.4876 - val_accuracy: 0.6468\n","\n","Epoch 00012: val_loss did not improve from 2.88928\n","Epoch 13/100\n","426/426 [==============================] - 96s 226ms/step - loss: 2.8870 - dice_coef: 0.4843 - accuracy: 0.6452 - val_loss: 2.8893 - val_dice_coef: 0.4837 - val_accuracy: 0.6468\n","\n","Epoch 00013: val_loss improved from 2.88928 to 2.88926, saving model to Checkpoint.h5\n","Epoch 14/100\n","426/426 [==============================] - 96s 226ms/step - loss: 2.8851 - dice_coef: 0.4879 - accuracy: 0.6530 - val_loss: 2.8894 - val_dice_coef: 0.4888 - val_accuracy: 0.6468\n","\n","Epoch 00014: val_loss did not improve from 2.88926\n","Epoch 15/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8839 - dice_coef: 0.4921 - accuracy: 0.6554 - val_loss: 2.8895 - val_dice_coef: 0.4898 - val_accuracy: 0.6468\n","\n","Epoch 00015: val_loss did not improve from 2.88926\n","Epoch 16/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8824 - dice_coef: 0.4986 - accuracy: 0.6616 - val_loss: 2.8895 - val_dice_coef: 0.4902 - val_accuracy: 0.6468\n","\n","Epoch 00016: val_loss did not improve from 2.88926\n","Epoch 17/100\n","426/426 [==============================] - 97s 229ms/step - loss: 2.8841 - dice_coef: 0.4922 - accuracy: 0.6542 - val_loss: 2.8893 - val_dice_coef: 0.4885 - val_accuracy: 0.6468\n","\n","Epoch 00017: val_loss did not improve from 2.88926\n","Epoch 18/100\n","426/426 [==============================] - 97s 227ms/step - loss: 2.8873 - dice_coef: 0.4915 - accuracy: 0.6531 - val_loss: 2.8896 - val_dice_coef: 0.4876 - val_accuracy: 0.6468\n","\n","Epoch 00018: val_loss did not improve from 2.88926\n","Epoch 19/100\n","426/426 [==============================] - 96s 226ms/step - loss: 2.8867 - dice_coef: 0.4864 - accuracy: 0.6470 - val_loss: 2.8894 - val_dice_coef: 0.4844 - val_accuracy: 0.6468\n","\n","Epoch 00019: val_loss did not improve from 2.88926\n","Epoch 20/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8862 - dice_coef: 0.4877 - accuracy: 0.6506 - val_loss: 2.8893 - val_dice_coef: 0.4856 - val_accuracy: 0.6468\n","\n","Epoch 00020: val_loss did not improve from 2.88926\n","Epoch 21/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8850 - dice_coef: 0.4881 - accuracy: 0.6520 - val_loss: 2.8893 - val_dice_coef: 0.4879 - val_accuracy: 0.6468\n","\n","Epoch 00021: val_loss did not improve from 2.88926\n","Epoch 22/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8845 - dice_coef: 0.4921 - accuracy: 0.6557 - val_loss: 2.8895 - val_dice_coef: 0.4897 - val_accuracy: 0.6468\n","\n","Epoch 00022: val_loss did not improve from 2.88926\n","Epoch 23/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8858 - dice_coef: 0.4875 - accuracy: 0.6492 - val_loss: 2.8896 - val_dice_coef: 0.4885 - val_accuracy: 0.6468\n","\n","Epoch 00023: val_loss did not improve from 2.88926\n","Epoch 24/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8843 - dice_coef: 0.4925 - accuracy: 0.6548 - val_loss: 2.8894 - val_dice_coef: 0.4875 - val_accuracy: 0.6468\n","\n","Epoch 00024: val_loss did not improve from 2.88926\n","Epoch 25/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8853 - dice_coef: 0.4906 - accuracy: 0.6534 - val_loss: 2.8894 - val_dice_coef: 0.4889 - val_accuracy: 0.6468\n","\n","Epoch 00025: val_loss did not improve from 2.88926\n","Epoch 26/100\n","426/426 [==============================] - 96s 226ms/step - loss: 2.8882 - dice_coef: 0.4858 - accuracy: 0.6469 - val_loss: 2.8893 - val_dice_coef: 0.4860 - val_accuracy: 0.6468\n","\n","Epoch 00026: val_loss did not improve from 2.88926\n","Epoch 27/100\n","426/426 [==============================] - 96s 226ms/step - loss: 2.8886 - dice_coef: 0.4823 - accuracy: 0.6435 - val_loss: 2.8893 - val_dice_coef: 0.4837 - val_accuracy: 0.6468\n","\n","Epoch 00027: val_loss did not improve from 2.88926\n","Epoch 28/100\n","426/426 [==============================] - 96s 226ms/step - loss: 2.8867 - dice_coef: 0.4878 - accuracy: 0.6517 - val_loss: 2.8893 - val_dice_coef: 0.4868 - val_accuracy: 0.6468\n","\n","Epoch 00028: val_loss did not improve from 2.88926\n","Epoch 29/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8848 - dice_coef: 0.4899 - accuracy: 0.6531 - val_loss: 2.8894 - val_dice_coef: 0.4876 - val_accuracy: 0.6468\n","\n","Epoch 00029: val_loss did not improve from 2.88926\n","Epoch 30/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8867 - dice_coef: 0.4869 - accuracy: 0.6474 - val_loss: 2.8897 - val_dice_coef: 0.4847 - val_accuracy: 0.6468\n","\n","Epoch 00030: val_loss did not improve from 2.88926\n","Epoch 31/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8836 - dice_coef: 0.4970 - accuracy: 0.6631 - val_loss: 2.8896 - val_dice_coef: 0.4918 - val_accuracy: 0.6468\n","\n","Epoch 00031: val_loss did not improve from 2.88926\n","Epoch 32/100\n","426/426 [==============================] - 96s 224ms/step - loss: 2.8859 - dice_coef: 0.4895 - accuracy: 0.6496 - val_loss: 2.8893 - val_dice_coef: 0.4863 - val_accuracy: 0.6468\n","\n","Epoch 00032: val_loss did not improve from 2.88926\n","Epoch 33/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8876 - dice_coef: 0.4822 - accuracy: 0.6430 - val_loss: 2.8893 - val_dice_coef: 0.4829 - val_accuracy: 0.6468\n","\n","Epoch 00033: val_loss did not improve from 2.88926\n","Epoch 34/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8849 - dice_coef: 0.4832 - accuracy: 0.6485 - val_loss: 2.8893 - val_dice_coef: 0.4878 - val_accuracy: 0.6468\n","\n","Epoch 00034: val_loss did not improve from 2.88926\n","Epoch 35/100\n","426/426 [==============================] - 96s 224ms/step - loss: 2.8882 - dice_coef: 0.4869 - accuracy: 0.6491 - val_loss: 2.8893 - val_dice_coef: 0.4870 - val_accuracy: 0.6468\n","\n","Epoch 00035: val_loss did not improve from 2.88926\n","Epoch 36/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8861 - dice_coef: 0.4875 - accuracy: 0.6503 - val_loss: 2.8893 - val_dice_coef: 0.4868 - val_accuracy: 0.6468\n","\n","Epoch 00036: val_loss did not improve from 2.88926\n","Epoch 37/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8817 - dice_coef: 0.4924 - accuracy: 0.6563 - val_loss: 2.8894 - val_dice_coef: 0.4883 - val_accuracy: 0.6468\n","\n","Epoch 00037: val_loss did not improve from 2.88926\n","Epoch 38/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8851 - dice_coef: 0.4943 - accuracy: 0.6577 - val_loss: 2.8894 - val_dice_coef: 0.4898 - val_accuracy: 0.6468\n","\n","Epoch 00038: val_loss did not improve from 2.88926\n","Epoch 39/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8851 - dice_coef: 0.4924 - accuracy: 0.6551 - val_loss: 2.8893 - val_dice_coef: 0.4892 - val_accuracy: 0.6468\n","\n","Epoch 00039: val_loss did not improve from 2.88926\n","Epoch 40/100\n","426/426 [==============================] - 96s 226ms/step - loss: 2.8832 - dice_coef: 0.4956 - accuracy: 0.6593 - val_loss: 2.8895 - val_dice_coef: 0.4910 - val_accuracy: 0.6468\n","\n","Epoch 00040: val_loss did not improve from 2.88926\n","Epoch 41/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8870 - dice_coef: 0.4891 - accuracy: 0.6495 - val_loss: 2.8894 - val_dice_coef: 0.4872 - val_accuracy: 0.6468\n","\n","Epoch 00041: val_loss did not improve from 2.88926\n","Epoch 42/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8843 - dice_coef: 0.4934 - accuracy: 0.6577 - val_loss: 2.8895 - val_dice_coef: 0.4907 - val_accuracy: 0.6468\n","\n","Epoch 00042: val_loss did not improve from 2.88926\n","Epoch 43/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8848 - dice_coef: 0.4941 - accuracy: 0.6555 - val_loss: 2.8895 - val_dice_coef: 0.4884 - val_accuracy: 0.6468\n","\n","Epoch 00043: val_loss did not improve from 2.88926\n","Epoch 44/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8831 - dice_coef: 0.4941 - accuracy: 0.6585 - val_loss: 2.8896 - val_dice_coef: 0.4916 - val_accuracy: 0.6468\n","\n","Epoch 00044: val_loss did not improve from 2.88926\n","Epoch 45/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8867 - dice_coef: 0.4887 - accuracy: 0.6485 - val_loss: 2.8893 - val_dice_coef: 0.4861 - val_accuracy: 0.6468\n","\n","Epoch 00045: val_loss did not improve from 2.88926\n","Epoch 46/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8873 - dice_coef: 0.4856 - accuracy: 0.6483 - val_loss: 2.8893 - val_dice_coef: 0.4865 - val_accuracy: 0.6468\n","\n","Epoch 00046: val_loss did not improve from 2.88926\n","Epoch 47/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8881 - dice_coef: 0.4853 - accuracy: 0.6474 - val_loss: 2.8893 - val_dice_coef: 0.4855 - val_accuracy: 0.6468\n","\n","Epoch 00047: val_loss did not improve from 2.88926\n","Epoch 48/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8843 - dice_coef: 0.4888 - accuracy: 0.6531 - val_loss: 2.8893 - val_dice_coef: 0.4879 - val_accuracy: 0.6468\n","\n","Epoch 00048: val_loss did not improve from 2.88926\n","Epoch 49/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8846 - dice_coef: 0.4904 - accuracy: 0.6540 - val_loss: 2.8894 - val_dice_coef: 0.4893 - val_accuracy: 0.6468\n","\n","Epoch 00049: val_loss did not improve from 2.88926\n","Epoch 50/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8828 - dice_coef: 0.4979 - accuracy: 0.6610 - val_loss: 2.8895 - val_dice_coef: 0.4896 - val_accuracy: 0.6468\n","\n","Epoch 00050: val_loss did not improve from 2.88926\n","Epoch 51/100\n","426/426 [==============================] - 96s 224ms/step - loss: 2.8850 - dice_coef: 0.4903 - accuracy: 0.6527 - val_loss: 2.8894 - val_dice_coef: 0.4892 - val_accuracy: 0.6468\n","\n","Epoch 00051: val_loss did not improve from 2.88926\n","Epoch 52/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8855 - dice_coef: 0.4889 - accuracy: 0.6510 - val_loss: 2.8894 - val_dice_coef: 0.4880 - val_accuracy: 0.6468\n","\n","Epoch 00052: val_loss did not improve from 2.88926\n","Epoch 53/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8835 - dice_coef: 0.4959 - accuracy: 0.6597 - val_loss: 2.8894 - val_dice_coef: 0.4898 - val_accuracy: 0.6468\n","\n","Epoch 00053: val_loss did not improve from 2.88926\n","Epoch 54/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8870 - dice_coef: 0.4914 - accuracy: 0.6535 - val_loss: 2.8893 - val_dice_coef: 0.4888 - val_accuracy: 0.6468\n","\n","Epoch 00054: val_loss did not improve from 2.88926\n","Epoch 55/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8880 - dice_coef: 0.4828 - accuracy: 0.6432 - val_loss: 2.8892 - val_dice_coef: 0.4846 - val_accuracy: 0.6468\n","\n","Epoch 00055: val_loss improved from 2.88926 to 2.88924, saving model to Checkpoint.h5\n","Epoch 56/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8855 - dice_coef: 0.4887 - accuracy: 0.6534 - val_loss: 2.8893 - val_dice_coef: 0.4887 - val_accuracy: 0.6468\n","\n","Epoch 00056: val_loss did not improve from 2.88924\n","Epoch 57/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8860 - dice_coef: 0.4909 - accuracy: 0.6530 - val_loss: 2.8893 - val_dice_coef: 0.4875 - val_accuracy: 0.6468\n","\n","Epoch 00057: val_loss did not improve from 2.88924\n","Epoch 58/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8853 - dice_coef: 0.4872 - accuracy: 0.6491 - val_loss: 2.8893 - val_dice_coef: 0.4854 - val_accuracy: 0.6468\n","\n","Epoch 00058: val_loss did not improve from 2.88924\n","Epoch 59/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8834 - dice_coef: 0.4884 - accuracy: 0.6528 - val_loss: 2.8893 - val_dice_coef: 0.4879 - val_accuracy: 0.6468\n","\n","Epoch 00059: val_loss did not improve from 2.88924\n","Epoch 60/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8872 - dice_coef: 0.4859 - accuracy: 0.6479 - val_loss: 2.8892 - val_dice_coef: 0.4864 - val_accuracy: 0.6468\n","\n","Epoch 00060: val_loss did not improve from 2.88924\n","Epoch 61/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8857 - dice_coef: 0.4881 - accuracy: 0.6517 - val_loss: 2.8893 - val_dice_coef: 0.4879 - val_accuracy: 0.6468\n","\n","Epoch 00061: val_loss did not improve from 2.88924\n","Epoch 62/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8852 - dice_coef: 0.4893 - accuracy: 0.6526 - val_loss: 2.8893 - val_dice_coef: 0.4886 - val_accuracy: 0.6468\n","\n","Epoch 00062: val_loss did not improve from 2.88924\n","Epoch 63/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8853 - dice_coef: 0.4883 - accuracy: 0.6500 - val_loss: 2.8893 - val_dice_coef: 0.4864 - val_accuracy: 0.6468\n","\n","Epoch 00063: val_loss did not improve from 2.88924\n","Epoch 64/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8866 - dice_coef: 0.4875 - accuracy: 0.6508 - val_loss: 2.8893 - val_dice_coef: 0.4879 - val_accuracy: 0.6468\n","\n","Epoch 00064: val_loss did not improve from 2.88924\n","Epoch 65/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8808 - dice_coef: 0.4971 - accuracy: 0.6629 - val_loss: 2.8897 - val_dice_coef: 0.4931 - val_accuracy: 0.6468\n","\n","Epoch 00065: val_loss did not improve from 2.88924\n","Epoch 66/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8842 - dice_coef: 0.4904 - accuracy: 0.6506 - val_loss: 2.8893 - val_dice_coef: 0.4872 - val_accuracy: 0.6468\n","\n","Epoch 00066: val_loss did not improve from 2.88924\n","Epoch 67/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8863 - dice_coef: 0.4885 - accuracy: 0.6508 - val_loss: 2.8893 - val_dice_coef: 0.4865 - val_accuracy: 0.6468\n","\n","Epoch 00067: val_loss did not improve from 2.88924\n","Epoch 68/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8846 - dice_coef: 0.4891 - accuracy: 0.6522 - val_loss: 2.8894 - val_dice_coef: 0.4874 - val_accuracy: 0.6468\n","\n","Epoch 00068: val_loss did not improve from 2.88924\n","Epoch 69/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8832 - dice_coef: 0.4945 - accuracy: 0.6589 - val_loss: 2.8894 - val_dice_coef: 0.4904 - val_accuracy: 0.6468\n","\n","Epoch 00069: val_loss did not improve from 2.88924\n","Epoch 70/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8875 - dice_coef: 0.4825 - accuracy: 0.6427 - val_loss: 2.8893 - val_dice_coef: 0.4857 - val_accuracy: 0.6468\n","\n","Epoch 00070: val_loss did not improve from 2.88924\n","Epoch 71/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8856 - dice_coef: 0.4893 - accuracy: 0.6532 - val_loss: 2.8893 - val_dice_coef: 0.4879 - val_accuracy: 0.6468\n","\n","Epoch 00071: val_loss did not improve from 2.88924\n","Epoch 72/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8850 - dice_coef: 0.4869 - accuracy: 0.6492 - val_loss: 2.8893 - val_dice_coef: 0.4871 - val_accuracy: 0.6468\n","\n","Epoch 00072: val_loss did not improve from 2.88924\n","Epoch 73/100\n","426/426 [==============================] - 96s 224ms/step - loss: 2.8852 - dice_coef: 0.4879 - accuracy: 0.6507 - val_loss: 2.8893 - val_dice_coef: 0.4874 - val_accuracy: 0.6468\n","\n","Epoch 00073: val_loss did not improve from 2.88924\n","Epoch 74/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8862 - dice_coef: 0.4870 - accuracy: 0.6497 - val_loss: 2.8894 - val_dice_coef: 0.4877 - val_accuracy: 0.6468\n","\n","Epoch 00074: val_loss did not improve from 2.88924\n","Epoch 75/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8858 - dice_coef: 0.4883 - accuracy: 0.6516 - val_loss: 2.8894 - val_dice_coef: 0.4890 - val_accuracy: 0.6468\n","\n","Epoch 00075: val_loss did not improve from 2.88924\n","Epoch 76/100\n","426/426 [==============================] - 96s 224ms/step - loss: 2.8872 - dice_coef: 0.4887 - accuracy: 0.6502 - val_loss: 2.8893 - val_dice_coef: 0.4868 - val_accuracy: 0.6468\n","\n","Epoch 00076: val_loss did not improve from 2.88924\n","Epoch 77/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8876 - dice_coef: 0.4873 - accuracy: 0.6491 - val_loss: 2.8892 - val_dice_coef: 0.4851 - val_accuracy: 0.6468\n","\n","Epoch 00077: val_loss did not improve from 2.88924\n","Epoch 78/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8874 - dice_coef: 0.4843 - accuracy: 0.6460 - val_loss: 2.8893 - val_dice_coef: 0.4837 - val_accuracy: 0.6468\n","\n","Epoch 00078: val_loss did not improve from 2.88924\n","Epoch 79/100\n","426/426 [==============================] - 96s 224ms/step - loss: 2.8846 - dice_coef: 0.4916 - accuracy: 0.6568 - val_loss: 2.8893 - val_dice_coef: 0.4881 - val_accuracy: 0.6468\n","\n","Epoch 00079: val_loss did not improve from 2.88924\n","Epoch 80/100\n","426/426 [==============================] - 96s 224ms/step - loss: 2.8835 - dice_coef: 0.4949 - accuracy: 0.6589 - val_loss: 2.8894 - val_dice_coef: 0.4905 - val_accuracy: 0.6468\n","\n","Epoch 00080: val_loss did not improve from 2.88924\n","Epoch 81/100\n","426/426 [==============================] - 95s 224ms/step - loss: 2.8863 - dice_coef: 0.4895 - accuracy: 0.6510 - val_loss: 2.8893 - val_dice_coef: 0.4877 - val_accuracy: 0.6468\n","\n","Epoch 00081: val_loss did not improve from 2.88924\n","Epoch 82/100\n","426/426 [==============================] - 95s 224ms/step - loss: 2.8861 - dice_coef: 0.4903 - accuracy: 0.6530 - val_loss: 2.8893 - val_dice_coef: 0.4879 - val_accuracy: 0.6468\n","\n","Epoch 00082: val_loss did not improve from 2.88924\n","Epoch 83/100\n","426/426 [==============================] - 95s 224ms/step - loss: 2.8851 - dice_coef: 0.4917 - accuracy: 0.6548 - val_loss: 2.8893 - val_dice_coef: 0.4887 - val_accuracy: 0.6468\n","\n","Epoch 00083: val_loss did not improve from 2.88924\n","Epoch 84/100\n","426/426 [==============================] - 95s 224ms/step - loss: 2.8874 - dice_coef: 0.4862 - accuracy: 0.6470 - val_loss: 2.8892 - val_dice_coef: 0.4853 - val_accuracy: 0.6468\n","\n","Epoch 00084: val_loss did not improve from 2.88924\n","Epoch 85/100\n","426/426 [==============================] - 95s 224ms/step - loss: 2.8856 - dice_coef: 0.4880 - accuracy: 0.6515 - val_loss: 2.8892 - val_dice_coef: 0.4863 - val_accuracy: 0.6468\n","\n","Epoch 00085: val_loss did not improve from 2.88924\n","Epoch 86/100\n","426/426 [==============================] - 95s 224ms/step - loss: 2.8854 - dice_coef: 0.4876 - accuracy: 0.6509 - val_loss: 2.8893 - val_dice_coef: 0.4876 - val_accuracy: 0.6468\n","\n","Epoch 00086: val_loss did not improve from 2.88924\n","Epoch 87/100\n","426/426 [==============================] - 96s 224ms/step - loss: 2.8863 - dice_coef: 0.4873 - accuracy: 0.6490 - val_loss: 2.8893 - val_dice_coef: 0.4853 - val_accuracy: 0.6468\n","\n","Epoch 00087: val_loss did not improve from 2.88924\n","Epoch 88/100\n","426/426 [==============================] - 95s 224ms/step - loss: 2.8867 - dice_coef: 0.4853 - accuracy: 0.6482 - val_loss: 2.8893 - val_dice_coef: 0.4859 - val_accuracy: 0.6468\n","\n","Epoch 00088: val_loss did not improve from 2.88924\n","Epoch 89/100\n","426/426 [==============================] - 95s 224ms/step - loss: 2.8859 - dice_coef: 0.4881 - accuracy: 0.6512 - val_loss: 2.8893 - val_dice_coef: 0.4866 - val_accuracy: 0.6468\n","\n","Epoch 00089: val_loss did not improve from 2.88924\n","Epoch 90/100\n","426/426 [==============================] - 95s 224ms/step - loss: 2.8870 - dice_coef: 0.4866 - accuracy: 0.6487 - val_loss: 2.8892 - val_dice_coef: 0.4854 - val_accuracy: 0.6468\n","\n","Epoch 00090: val_loss did not improve from 2.88924\n","Epoch 91/100\n","426/426 [==============================] - 95s 224ms/step - loss: 2.8891 - dice_coef: 0.4774 - accuracy: 0.6369 - val_loss: 2.8894 - val_dice_coef: 0.4800 - val_accuracy: 0.6468\n","\n","Epoch 00091: val_loss did not improve from 2.88924\n","Epoch 92/100\n","426/426 [==============================] - 95s 224ms/step - loss: 2.8875 - dice_coef: 0.4811 - accuracy: 0.6457 - val_loss: 2.8893 - val_dice_coef: 0.4840 - val_accuracy: 0.6468\n","\n","Epoch 00092: val_loss did not improve from 2.88924\n","Epoch 93/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8842 - dice_coef: 0.4915 - accuracy: 0.6573 - val_loss: 2.8895 - val_dice_coef: 0.4904 - val_accuracy: 0.6468\n","\n","Epoch 00093: val_loss did not improve from 2.88924\n","Epoch 94/100\n","426/426 [==============================] - 96s 224ms/step - loss: 2.8860 - dice_coef: 0.4926 - accuracy: 0.6542 - val_loss: 2.8894 - val_dice_coef: 0.4879 - val_accuracy: 0.6468\n","\n","Epoch 00094: val_loss did not improve from 2.88924\n","Epoch 95/100\n","426/426 [==============================] - 96s 224ms/step - loss: 2.8890 - dice_coef: 0.4828 - accuracy: 0.6438 - val_loss: 2.8892 - val_dice_coef: 0.4855 - val_accuracy: 0.6468\n","\n","Epoch 00095: val_loss did not improve from 2.88924\n","Epoch 96/100\n","426/426 [==============================] - 96s 224ms/step - loss: 2.8825 - dice_coef: 0.4940 - accuracy: 0.6592 - val_loss: 2.8894 - val_dice_coef: 0.4894 - val_accuracy: 0.6468\n","\n","Epoch 00096: val_loss did not improve from 2.88924\n","Epoch 97/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8850 - dice_coef: 0.4912 - accuracy: 0.6524 - val_loss: 2.8893 - val_dice_coef: 0.4859 - val_accuracy: 0.6468\n","\n","Epoch 00097: val_loss did not improve from 2.88924\n","Epoch 98/100\n","426/426 [==============================] - 95s 224ms/step - loss: 2.8839 - dice_coef: 0.4915 - accuracy: 0.6552 - val_loss: 2.8893 - val_dice_coef: 0.4876 - val_accuracy: 0.6468\n","\n","Epoch 00098: val_loss did not improve from 2.88924\n","Epoch 99/100\n","426/426 [==============================] - 95s 224ms/step - loss: 2.8848 - dice_coef: 0.4927 - accuracy: 0.6561 - val_loss: 2.8894 - val_dice_coef: 0.4893 - val_accuracy: 0.6468\n","\n","Epoch 00099: val_loss did not improve from 2.88924\n","Epoch 100/100\n","426/426 [==============================] - 95s 224ms/step - loss: 2.8860 - dice_coef: 0.4918 - accuracy: 0.6533 - val_loss: 2.8893 - val_dice_coef: 0.4870 - val_accuracy: 0.6468\n","\n","Epoch 00100: val_loss did not improve from 2.88924\n","TRAIN: [0, 1, 2, 4, 5, 7, 8] TEST: [3, 6] FOLD: 1\n","(None, 15, 15, 1)\n","(None, 30, 30, 1)\n","-----------fold 1--------------\n","Epoch 1/100\n","420/420 [==============================] - 97s 230ms/step - loss: 122.2400 - dice_coef: 0.4587 - accuracy: 0.6358 - val_loss: 52.6570 - val_dice_coef: 0.4573 - val_accuracy: 0.6570\n","\n","Epoch 00001: val_loss improved from inf to 52.65702, saving model to Checkpoint.h5\n","Epoch 2/100\n","420/420 [==============================] - 95s 225ms/step - loss: 28.2255 - dice_coef: 0.4757 - accuracy: 0.6575 - val_loss: 13.4503 - val_dice_coef: 0.4825 - val_accuracy: 0.6570\n","\n","Epoch 00002: val_loss improved from 52.65702 to 13.45031, saving model to Checkpoint.h5\n","Epoch 3/100\n","420/420 [==============================] - 95s 226ms/step - loss: 8.2619 - dice_coef: 0.4871 - accuracy: 0.6581 - val_loss: 5.1270 - val_dice_coef: 0.4882 - val_accuracy: 0.6570\n","\n","Epoch 00003: val_loss improved from 13.45031 to 5.12702, saving model to Checkpoint.h5\n","Epoch 4/100\n","420/420 [==============================] - 95s 225ms/step - loss: 4.0255 - dice_coef: 0.4881 - accuracy: 0.6553 - val_loss: 3.3598 - val_dice_coef: 0.4923 - val_accuracy: 0.6570\n","\n","Epoch 00004: val_loss improved from 5.12702 to 3.35977, saving model to Checkpoint.h5\n","Epoch 5/100\n","420/420 [==============================] - 95s 225ms/step - loss: 3.1241 - dice_coef: 0.4890 - accuracy: 0.6535 - val_loss: 2.9846 - val_dice_coef: 0.4930 - val_accuracy: 0.6570\n","\n","Epoch 00005: val_loss improved from 3.35977 to 2.98460, saving model to Checkpoint.h5\n","Epoch 6/100\n","420/420 [==============================] - 95s 226ms/step - loss: 2.9323 - dice_coef: 0.4928 - accuracy: 0.6561 - val_loss: 2.9045 - val_dice_coef: 0.4940 - val_accuracy: 0.6570\n","\n","Epoch 00006: val_loss improved from 2.98460 to 2.90446, saving model to Checkpoint.h5\n","Epoch 7/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8917 - dice_coef: 0.4958 - accuracy: 0.6585 - val_loss: 2.8876 - val_dice_coef: 0.4948 - val_accuracy: 0.6570\n","\n","Epoch 00007: val_loss improved from 2.90446 to 2.88756, saving model to Checkpoint.h5\n","Epoch 8/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8838 - dice_coef: 0.4962 - accuracy: 0.6589 - val_loss: 2.8840 - val_dice_coef: 0.4955 - val_accuracy: 0.6570\n","\n","Epoch 00008: val_loss improved from 2.88756 to 2.88397, saving model to Checkpoint.h5\n","Epoch 9/100\n","420/420 [==============================] - 95s 226ms/step - loss: 2.8821 - dice_coef: 0.4952 - accuracy: 0.6577 - val_loss: 2.8832 - val_dice_coef: 0.4965 - val_accuracy: 0.6570\n","\n","Epoch 00009: val_loss improved from 2.88397 to 2.88322, saving model to Checkpoint.h5\n","Epoch 10/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8810 - dice_coef: 0.4948 - accuracy: 0.6552 - val_loss: 2.8831 - val_dice_coef: 0.4930 - val_accuracy: 0.6570\n","\n","Epoch 00010: val_loss improved from 2.88322 to 2.88313, saving model to Checkpoint.h5\n","Epoch 11/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8803 - dice_coef: 0.4956 - accuracy: 0.6598 - val_loss: 2.8829 - val_dice_coef: 0.4983 - val_accuracy: 0.6570\n","\n","Epoch 00011: val_loss improved from 2.88313 to 2.88293, saving model to Checkpoint.h5\n","Epoch 12/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8807 - dice_coef: 0.4954 - accuracy: 0.6560 - val_loss: 2.8829 - val_dice_coef: 0.4956 - val_accuracy: 0.6570\n","\n","Epoch 00012: val_loss improved from 2.88293 to 2.88290, saving model to Checkpoint.h5\n","Epoch 13/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8811 - dice_coef: 0.4949 - accuracy: 0.6579 - val_loss: 2.8831 - val_dice_coef: 0.4976 - val_accuracy: 0.6570\n","\n","Epoch 00013: val_loss did not improve from 2.88290\n","Epoch 14/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8800 - dice_coef: 0.5007 - accuracy: 0.6628 - val_loss: 2.8829 - val_dice_coef: 0.4983 - val_accuracy: 0.6570\n","\n","Epoch 00014: val_loss did not improve from 2.88290\n","Epoch 15/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8788 - dice_coef: 0.5019 - accuracy: 0.6627 - val_loss: 2.8829 - val_dice_coef: 0.4959 - val_accuracy: 0.6570\n","\n","Epoch 00015: val_loss did not improve from 2.88290\n","Epoch 16/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8779 - dice_coef: 0.5032 - accuracy: 0.6672 - val_loss: 2.8830 - val_dice_coef: 0.5008 - val_accuracy: 0.6570\n","\n","Epoch 00016: val_loss did not improve from 2.88290\n","Epoch 17/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8827 - dice_coef: 0.4935 - accuracy: 0.6525 - val_loss: 2.8828 - val_dice_coef: 0.4965 - val_accuracy: 0.6570\n","\n","Epoch 00017: val_loss improved from 2.88290 to 2.88279, saving model to Checkpoint.h5\n","Epoch 18/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8823 - dice_coef: 0.4928 - accuracy: 0.6537 - val_loss: 2.8830 - val_dice_coef: 0.4948 - val_accuracy: 0.6570\n","\n","Epoch 00018: val_loss did not improve from 2.88279\n","Epoch 19/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8795 - dice_coef: 0.4985 - accuracy: 0.6612 - val_loss: 2.8829 - val_dice_coef: 0.4969 - val_accuracy: 0.6570\n","\n","Epoch 00019: val_loss did not improve from 2.88279\n","Epoch 20/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8809 - dice_coef: 0.4954 - accuracy: 0.6570 - val_loss: 2.8829 - val_dice_coef: 0.4970 - val_accuracy: 0.6570\n","\n","Epoch 00020: val_loss did not improve from 2.88279\n","Epoch 21/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8810 - dice_coef: 0.4944 - accuracy: 0.6561 - val_loss: 2.8830 - val_dice_coef: 0.4965 - val_accuracy: 0.6570\n","\n","Epoch 00021: val_loss did not improve from 2.88279\n","Epoch 22/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8817 - dice_coef: 0.4954 - accuracy: 0.6565 - val_loss: 2.8831 - val_dice_coef: 0.4950 - val_accuracy: 0.6570\n","\n","Epoch 00022: val_loss did not improve from 2.88279\n","Epoch 23/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8805 - dice_coef: 0.4964 - accuracy: 0.6601 - val_loss: 2.8830 - val_dice_coef: 0.4993 - val_accuracy: 0.6570\n","\n","Epoch 00023: val_loss did not improve from 2.88279\n","Epoch 24/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8809 - dice_coef: 0.4972 - accuracy: 0.6579 - val_loss: 2.8829 - val_dice_coef: 0.4971 - val_accuracy: 0.6570\n","\n","Epoch 00024: val_loss did not improve from 2.88279\n","Epoch 25/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8818 - dice_coef: 0.4949 - accuracy: 0.6562 - val_loss: 2.8829 - val_dice_coef: 0.4962 - val_accuracy: 0.6570\n","\n","Epoch 00025: val_loss did not improve from 2.88279\n","Epoch 26/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8832 - dice_coef: 0.4920 - accuracy: 0.6525 - val_loss: 2.8829 - val_dice_coef: 0.4946 - val_accuracy: 0.6570\n","\n","Epoch 00026: val_loss did not improve from 2.88279\n","Epoch 27/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8812 - dice_coef: 0.4936 - accuracy: 0.6560 - val_loss: 2.8829 - val_dice_coef: 0.4961 - val_accuracy: 0.6570\n","\n","Epoch 00027: val_loss did not improve from 2.88279\n","Epoch 28/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8823 - dice_coef: 0.4919 - accuracy: 0.6528 - val_loss: 2.8829 - val_dice_coef: 0.4948 - val_accuracy: 0.6570\n","\n","Epoch 00028: val_loss did not improve from 2.88279\n","Epoch 29/100\n","420/420 [==============================] - 95s 226ms/step - loss: 2.8785 - dice_coef: 0.4994 - accuracy: 0.6621 - val_loss: 2.8829 - val_dice_coef: 0.4959 - val_accuracy: 0.6570\n","\n","Epoch 00029: val_loss did not improve from 2.88279\n","Epoch 30/100\n","420/420 [==============================] - 95s 226ms/step - loss: 2.8808 - dice_coef: 0.4941 - accuracy: 0.6546 - val_loss: 2.8830 - val_dice_coef: 0.4934 - val_accuracy: 0.6570\n","\n","Epoch 00030: val_loss did not improve from 2.88279\n","Epoch 31/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8833 - dice_coef: 0.4934 - accuracy: 0.6549 - val_loss: 2.8831 - val_dice_coef: 0.4935 - val_accuracy: 0.6570\n","\n","Epoch 00031: val_loss did not improve from 2.88279\n","Epoch 32/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8835 - dice_coef: 0.4903 - accuracy: 0.6535 - val_loss: 2.8833 - val_dice_coef: 0.4956 - val_accuracy: 0.6570\n","\n","Epoch 00032: val_loss did not improve from 2.88279\n","Epoch 33/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8789 - dice_coef: 0.4995 - accuracy: 0.6629 - val_loss: 2.8830 - val_dice_coef: 0.4987 - val_accuracy: 0.6570\n","\n","Epoch 00033: val_loss did not improve from 2.88279\n","Epoch 34/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8834 - dice_coef: 0.4908 - accuracy: 0.6504 - val_loss: 2.8833 - val_dice_coef: 0.4935 - val_accuracy: 0.6570\n","\n","Epoch 00034: val_loss did not improve from 2.88279\n","Epoch 35/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8829 - dice_coef: 0.4913 - accuracy: 0.6530 - val_loss: 2.8833 - val_dice_coef: 0.4932 - val_accuracy: 0.6570\n","\n","Epoch 00035: val_loss did not improve from 2.88279\n","Epoch 36/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8801 - dice_coef: 0.4923 - accuracy: 0.6541 - val_loss: 2.8830 - val_dice_coef: 0.4924 - val_accuracy: 0.6570\n","\n","Epoch 00036: val_loss did not improve from 2.88279\n","Epoch 37/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8803 - dice_coef: 0.4962 - accuracy: 0.6596 - val_loss: 2.8831 - val_dice_coef: 0.4953 - val_accuracy: 0.6570\n","\n","Epoch 00037: val_loss did not improve from 2.88279\n","Epoch 38/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8826 - dice_coef: 0.4936 - accuracy: 0.6553 - val_loss: 2.8830 - val_dice_coef: 0.4957 - val_accuracy: 0.6570\n","\n","Epoch 00038: val_loss did not improve from 2.88279\n","Epoch 39/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8816 - dice_coef: 0.4929 - accuracy: 0.6540 - val_loss: 2.8831 - val_dice_coef: 0.4939 - val_accuracy: 0.6570\n","\n","Epoch 00039: val_loss did not improve from 2.88279\n","Epoch 40/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8821 - dice_coef: 0.4926 - accuracy: 0.6536 - val_loss: 2.8829 - val_dice_coef: 0.4931 - val_accuracy: 0.6570\n","\n","Epoch 00040: val_loss did not improve from 2.88279\n","Epoch 41/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8833 - dice_coef: 0.4878 - accuracy: 0.6494 - val_loss: 2.8830 - val_dice_coef: 0.4937 - val_accuracy: 0.6570\n","\n","Epoch 00041: val_loss did not improve from 2.88279\n","Epoch 42/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8826 - dice_coef: 0.4921 - accuracy: 0.6543 - val_loss: 2.8830 - val_dice_coef: 0.4953 - val_accuracy: 0.6570\n","\n","Epoch 00042: val_loss did not improve from 2.88279\n","Epoch 43/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8805 - dice_coef: 0.4952 - accuracy: 0.6588 - val_loss: 2.8832 - val_dice_coef: 0.4976 - val_accuracy: 0.6570\n","\n","Epoch 00043: val_loss did not improve from 2.88279\n","Epoch 44/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8826 - dice_coef: 0.4921 - accuracy: 0.6520 - val_loss: 2.8833 - val_dice_coef: 0.4935 - val_accuracy: 0.6570\n","\n","Epoch 00044: val_loss did not improve from 2.88279\n","Epoch 45/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8794 - dice_coef: 0.4994 - accuracy: 0.6628 - val_loss: 2.8830 - val_dice_coef: 0.4966 - val_accuracy: 0.6570\n","\n","Epoch 00045: val_loss did not improve from 2.88279\n","Epoch 46/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8799 - dice_coef: 0.4980 - accuracy: 0.6597 - val_loss: 2.8832 - val_dice_coef: 0.4953 - val_accuracy: 0.6570\n","\n","Epoch 00046: val_loss did not improve from 2.88279\n","Epoch 47/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8856 - dice_coef: 0.4849 - accuracy: 0.6448 - val_loss: 2.8832 - val_dice_coef: 0.4922 - val_accuracy: 0.6570\n","\n","Epoch 00047: val_loss did not improve from 2.88279\n","Epoch 48/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8822 - dice_coef: 0.4929 - accuracy: 0.6556 - val_loss: 2.8831 - val_dice_coef: 0.4939 - val_accuracy: 0.6570\n","\n","Epoch 00048: val_loss did not improve from 2.88279\n","Epoch 49/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8815 - dice_coef: 0.4946 - accuracy: 0.6567 - val_loss: 2.8829 - val_dice_coef: 0.4949 - val_accuracy: 0.6570\n","\n","Epoch 00049: val_loss did not improve from 2.88279\n","Epoch 50/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8819 - dice_coef: 0.4939 - accuracy: 0.6559 - val_loss: 2.8828 - val_dice_coef: 0.4962 - val_accuracy: 0.6570\n","\n","Epoch 00050: val_loss did not improve from 2.88279\n","Epoch 51/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8823 - dice_coef: 0.4888 - accuracy: 0.6480 - val_loss: 2.8831 - val_dice_coef: 0.4909 - val_accuracy: 0.6570\n","\n","Epoch 00051: val_loss did not improve from 2.88279\n","Epoch 52/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8827 - dice_coef: 0.4880 - accuracy: 0.6502 - val_loss: 2.8833 - val_dice_coef: 0.4919 - val_accuracy: 0.6570\n","\n","Epoch 00052: val_loss did not improve from 2.88279\n","Epoch 53/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8798 - dice_coef: 0.4981 - accuracy: 0.6632 - val_loss: 2.8830 - val_dice_coef: 0.4978 - val_accuracy: 0.6570\n","\n","Epoch 00053: val_loss did not improve from 2.88279\n","Epoch 54/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8812 - dice_coef: 0.4972 - accuracy: 0.6584 - val_loss: 2.8831 - val_dice_coef: 0.4960 - val_accuracy: 0.6570\n","\n","Epoch 00054: val_loss did not improve from 2.88279\n","Epoch 55/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8825 - dice_coef: 0.4920 - accuracy: 0.6527 - val_loss: 2.8830 - val_dice_coef: 0.4946 - val_accuracy: 0.6570\n","\n","Epoch 00055: val_loss did not improve from 2.88279\n","Epoch 56/100\n","420/420 [==============================] - 95s 226ms/step - loss: 2.8797 - dice_coef: 0.4963 - accuracy: 0.6601 - val_loss: 2.8830 - val_dice_coef: 0.4986 - val_accuracy: 0.6570\n","\n","Epoch 00056: val_loss did not improve from 2.88279\n","Epoch 57/100\n","420/420 [==============================] - 95s 226ms/step - loss: 2.8781 - dice_coef: 0.5024 - accuracy: 0.6652 - val_loss: 2.8832 - val_dice_coef: 0.4997 - val_accuracy: 0.6570\n","\n","Epoch 00057: val_loss did not improve from 2.88279\n","Epoch 58/100\n","420/420 [==============================] - 95s 226ms/step - loss: 2.8816 - dice_coef: 0.4969 - accuracy: 0.6572 - val_loss: 2.8830 - val_dice_coef: 0.4966 - val_accuracy: 0.6570\n","\n","Epoch 00058: val_loss did not improve from 2.88279\n","Epoch 59/100\n","420/420 [==============================] - 95s 226ms/step - loss: 2.8805 - dice_coef: 0.4991 - accuracy: 0.6607 - val_loss: 2.8828 - val_dice_coef: 0.4966 - val_accuracy: 0.6570\n","\n","Epoch 00059: val_loss did not improve from 2.88279\n","Epoch 60/100\n","420/420 [==============================] - 95s 226ms/step - loss: 2.8819 - dice_coef: 0.4929 - accuracy: 0.6531 - val_loss: 2.8831 - val_dice_coef: 0.4930 - val_accuracy: 0.6570\n","\n","Epoch 00060: val_loss did not improve from 2.88279\n","Epoch 61/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8833 - dice_coef: 0.4904 - accuracy: 0.6518 - val_loss: 2.8833 - val_dice_coef: 0.4921 - val_accuracy: 0.6570\n","\n","Epoch 00061: val_loss did not improve from 2.88279\n","Epoch 62/100\n","420/420 [==============================] - 95s 226ms/step - loss: 2.8792 - dice_coef: 0.4933 - accuracy: 0.6565 - val_loss: 2.8829 - val_dice_coef: 0.4949 - val_accuracy: 0.6570\n","\n","Epoch 00062: val_loss did not improve from 2.88279\n","Epoch 63/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8822 - dice_coef: 0.4927 - accuracy: 0.6538 - val_loss: 2.8829 - val_dice_coef: 0.4943 - val_accuracy: 0.6570\n","\n","Epoch 00063: val_loss did not improve from 2.88279\n","Epoch 64/100\n","420/420 [==============================] - 94s 224ms/step - loss: 2.8811 - dice_coef: 0.4961 - accuracy: 0.6586 - val_loss: 2.8829 - val_dice_coef: 0.4965 - val_accuracy: 0.6570\n","\n","Epoch 00064: val_loss did not improve from 2.88279\n","Epoch 65/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8805 - dice_coef: 0.4995 - accuracy: 0.6619 - val_loss: 2.8831 - val_dice_coef: 0.4975 - val_accuracy: 0.6570\n","\n","Epoch 00065: val_loss did not improve from 2.88279\n","Epoch 66/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8811 - dice_coef: 0.4962 - accuracy: 0.6576 - val_loss: 2.8829 - val_dice_coef: 0.4970 - val_accuracy: 0.6570\n","\n","Epoch 00066: val_loss did not improve from 2.88279\n","Epoch 67/100\n","420/420 [==============================] - 94s 224ms/step - loss: 2.8819 - dice_coef: 0.4902 - accuracy: 0.6504 - val_loss: 2.8829 - val_dice_coef: 0.4943 - val_accuracy: 0.6570\n","\n","Epoch 00067: val_loss did not improve from 2.88279\n","Epoch 68/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8830 - dice_coef: 0.4923 - accuracy: 0.6533 - val_loss: 2.8832 - val_dice_coef: 0.4931 - val_accuracy: 0.6570\n","\n","Epoch 00068: val_loss did not improve from 2.88279\n","Epoch 69/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8788 - dice_coef: 0.5021 - accuracy: 0.6673 - val_loss: 2.8830 - val_dice_coef: 0.4993 - val_accuracy: 0.6570\n","\n","Epoch 00069: val_loss did not improve from 2.88279\n","Epoch 70/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8785 - dice_coef: 0.4995 - accuracy: 0.6620 - val_loss: 2.8830 - val_dice_coef: 0.5005 - val_accuracy: 0.6570\n","\n","Epoch 00070: val_loss did not improve from 2.88279\n","Epoch 71/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8819 - dice_coef: 0.4956 - accuracy: 0.6549 - val_loss: 2.8831 - val_dice_coef: 0.4953 - val_accuracy: 0.6570\n","\n","Epoch 00071: val_loss did not improve from 2.88279\n","Epoch 72/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8798 - dice_coef: 0.4953 - accuracy: 0.6580 - val_loss: 2.8829 - val_dice_coef: 0.4968 - val_accuracy: 0.6570\n","\n","Epoch 00072: val_loss did not improve from 2.88279\n","Epoch 73/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8806 - dice_coef: 0.4965 - accuracy: 0.6580 - val_loss: 2.8829 - val_dice_coef: 0.4960 - val_accuracy: 0.6570\n","\n","Epoch 00073: val_loss did not improve from 2.88279\n","Epoch 74/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8824 - dice_coef: 0.4931 - accuracy: 0.6535 - val_loss: 2.8829 - val_dice_coef: 0.4941 - val_accuracy: 0.6570\n","\n","Epoch 00074: val_loss did not improve from 2.88279\n","Epoch 75/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8835 - dice_coef: 0.4925 - accuracy: 0.6547 - val_loss: 2.8831 - val_dice_coef: 0.4949 - val_accuracy: 0.6570\n","\n","Epoch 00075: val_loss did not improve from 2.88279\n","Epoch 76/100\n","420/420 [==============================] - 94s 224ms/step - loss: 2.8798 - dice_coef: 0.4973 - accuracy: 0.6598 - val_loss: 2.8830 - val_dice_coef: 0.4959 - val_accuracy: 0.6570\n","\n","Epoch 00076: val_loss did not improve from 2.88279\n","Epoch 77/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8815 - dice_coef: 0.4957 - accuracy: 0.6564 - val_loss: 2.8830 - val_dice_coef: 0.4936 - val_accuracy: 0.6570\n","\n","Epoch 00077: val_loss did not improve from 2.88279\n","Epoch 78/100\n","420/420 [==============================] - 94s 224ms/step - loss: 2.8807 - dice_coef: 0.4966 - accuracy: 0.6598 - val_loss: 2.8830 - val_dice_coef: 0.4969 - val_accuracy: 0.6570\n","\n","Epoch 00078: val_loss did not improve from 2.88279\n","Epoch 79/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8803 - dice_coef: 0.4966 - accuracy: 0.6588 - val_loss: 2.8829 - val_dice_coef: 0.4976 - val_accuracy: 0.6570\n","\n","Epoch 00079: val_loss did not improve from 2.88279\n","Epoch 80/100\n","420/420 [==============================] - 94s 224ms/step - loss: 2.8822 - dice_coef: 0.4940 - accuracy: 0.6551 - val_loss: 2.8830 - val_dice_coef: 0.4961 - val_accuracy: 0.6570\n","\n","Epoch 00080: val_loss did not improve from 2.88279\n","Epoch 81/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8820 - dice_coef: 0.4923 - accuracy: 0.6528 - val_loss: 2.8831 - val_dice_coef: 0.4935 - val_accuracy: 0.6570\n","\n","Epoch 00081: val_loss did not improve from 2.88279\n","Epoch 82/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8827 - dice_coef: 0.4916 - accuracy: 0.6535 - val_loss: 2.8831 - val_dice_coef: 0.4942 - val_accuracy: 0.6570\n","\n","Epoch 00082: val_loss did not improve from 2.88279\n","Epoch 83/100\n","420/420 [==============================] - 94s 224ms/step - loss: 2.8823 - dice_coef: 0.4925 - accuracy: 0.6551 - val_loss: 2.8830 - val_dice_coef: 0.4960 - val_accuracy: 0.6570\n","\n","Epoch 00083: val_loss did not improve from 2.88279\n","Epoch 84/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8844 - dice_coef: 0.4894 - accuracy: 0.6495 - val_loss: 2.8835 - val_dice_coef: 0.4923 - val_accuracy: 0.6570\n","\n","Epoch 00084: val_loss did not improve from 2.88279\n","Epoch 85/100\n","420/420 [==============================] - 94s 224ms/step - loss: 2.8807 - dice_coef: 0.4942 - accuracy: 0.6573 - val_loss: 2.8831 - val_dice_coef: 0.4943 - val_accuracy: 0.6570\n","\n","Epoch 00085: val_loss did not improve from 2.88279\n","Epoch 86/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8795 - dice_coef: 0.5012 - accuracy: 0.6642 - val_loss: 2.8831 - val_dice_coef: 0.4960 - val_accuracy: 0.6570\n","\n","Epoch 00086: val_loss did not improve from 2.88279\n","Epoch 87/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8814 - dice_coef: 0.4929 - accuracy: 0.6539 - val_loss: 2.8830 - val_dice_coef: 0.4947 - val_accuracy: 0.6570\n","\n","Epoch 00087: val_loss did not improve from 2.88279\n","Epoch 88/100\n","420/420 [==============================] - 95s 226ms/step - loss: 2.8798 - dice_coef: 0.4999 - accuracy: 0.6620 - val_loss: 2.8829 - val_dice_coef: 0.4954 - val_accuracy: 0.6570\n","\n","Epoch 00088: val_loss did not improve from 2.88279\n","Epoch 89/100\n","420/420 [==============================] - 95s 226ms/step - loss: 2.8817 - dice_coef: 0.4976 - accuracy: 0.6607 - val_loss: 2.8829 - val_dice_coef: 0.4981 - val_accuracy: 0.6570\n","\n","Epoch 00089: val_loss did not improve from 2.88279\n","Epoch 90/100\n","420/420 [==============================] - 95s 226ms/step - loss: 2.8817 - dice_coef: 0.4944 - accuracy: 0.6545 - val_loss: 2.8829 - val_dice_coef: 0.4953 - val_accuracy: 0.6570\n","\n","Epoch 00090: val_loss did not improve from 2.88279\n","Epoch 91/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8812 - dice_coef: 0.4992 - accuracy: 0.6627 - val_loss: 2.8830 - val_dice_coef: 0.4986 - val_accuracy: 0.6570\n","\n","Epoch 00091: val_loss did not improve from 2.88279\n","Epoch 92/100\n","420/420 [==============================] - 95s 226ms/step - loss: 2.8791 - dice_coef: 0.5011 - accuracy: 0.6632 - val_loss: 2.8831 - val_dice_coef: 0.4985 - val_accuracy: 0.6570\n","\n","Epoch 00092: val_loss did not improve from 2.88279\n","Epoch 93/100\n","420/420 [==============================] - 95s 225ms/step - loss: 2.8804 - dice_coef: 0.4975 - accuracy: 0.6588 - val_loss: 2.8831 - val_dice_coef: 0.4972 - val_accuracy: 0.6570\n","\n","Epoch 00093: val_loss did not improve from 2.88279\n","Epoch 94/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8802 - dice_coef: 0.4984 - accuracy: 0.6597 - val_loss: 2.8829 - val_dice_coef: 0.4957 - val_accuracy: 0.6570\n","\n","Epoch 00094: val_loss did not improve from 2.88279\n","Epoch 95/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8822 - dice_coef: 0.4949 - accuracy: 0.6561 - val_loss: 2.8829 - val_dice_coef: 0.4959 - val_accuracy: 0.6570\n","\n","Epoch 00095: val_loss did not improve from 2.88279\n","Epoch 96/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8838 - dice_coef: 0.4935 - accuracy: 0.6543 - val_loss: 2.8831 - val_dice_coef: 0.4946 - val_accuracy: 0.6570\n","\n","Epoch 00096: val_loss did not improve from 2.88279\n","Epoch 97/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8823 - dice_coef: 0.4961 - accuracy: 0.6578 - val_loss: 2.8829 - val_dice_coef: 0.4953 - val_accuracy: 0.6570\n","\n","Epoch 00097: val_loss did not improve from 2.88279\n","Epoch 98/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8821 - dice_coef: 0.4954 - accuracy: 0.6572 - val_loss: 2.8829 - val_dice_coef: 0.4960 - val_accuracy: 0.6570\n","\n","Epoch 00098: val_loss did not improve from 2.88279\n","Epoch 99/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8801 - dice_coef: 0.4962 - accuracy: 0.6583 - val_loss: 2.8831 - val_dice_coef: 0.4962 - val_accuracy: 0.6570\n","\n","Epoch 00099: val_loss did not improve from 2.88279\n","Epoch 100/100\n","420/420 [==============================] - 94s 225ms/step - loss: 2.8811 - dice_coef: 0.4981 - accuracy: 0.6595 - val_loss: 2.8829 - val_dice_coef: 0.4955 - val_accuracy: 0.6570\n","\n","Epoch 00100: val_loss did not improve from 2.88279\n","TRAIN: [0, 1, 2, 3, 4, 6, 8] TEST: [5, 7] FOLD: 2\n","(None, 15, 15, 1)\n","(None, 30, 30, 1)\n","-----------fold 2--------------\n","Epoch 1/100\n","426/426 [==============================] - 98s 230ms/step - loss: 121.0879 - dice_coef: 0.5253 - accuracy: 0.6618 - val_loss: 51.6657 - val_dice_coef: 0.4932 - val_accuracy: 0.6672\n","\n","Epoch 00001: val_loss improved from inf to 51.66570, saving model to Checkpoint.h5\n","Epoch 2/100\n","426/426 [==============================] - 96s 225ms/step - loss: 27.5391 - dice_coef: 0.5049 - accuracy: 0.6751 - val_loss: 13.0497 - val_dice_coef: 0.4928 - val_accuracy: 0.6672\n","\n","Epoch 00002: val_loss improved from 51.66570 to 13.04968, saving model to Checkpoint.h5\n","Epoch 3/100\n","426/426 [==============================] - 96s 225ms/step - loss: 8.0349 - dice_coef: 0.5022 - accuracy: 0.6716 - val_loss: 5.0244 - val_dice_coef: 0.5018 - val_accuracy: 0.6672\n","\n","Epoch 00003: val_loss improved from 13.04968 to 5.02444, saving model to Checkpoint.h5\n","Epoch 4/100\n","426/426 [==============================] - 96s 225ms/step - loss: 3.9807 - dice_coef: 0.4941 - accuracy: 0.6625 - val_loss: 3.3553 - val_dice_coef: 0.4805 - val_accuracy: 0.6672\n","\n","Epoch 00004: val_loss improved from 5.02444 to 3.35528, saving model to Checkpoint.h5\n","Epoch 5/100\n","426/426 [==============================] - 96s 226ms/step - loss: 3.1343 - dice_coef: 0.5004 - accuracy: 0.6704 - val_loss: 3.0027 - val_dice_coef: 0.4964 - val_accuracy: 0.6672\n","\n","Epoch 00005: val_loss improved from 3.35528 to 3.00273, saving model to Checkpoint.h5\n","Epoch 6/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.9547 - dice_coef: 0.4979 - accuracy: 0.6706 - val_loss: 2.9244 - val_dice_coef: 0.4960 - val_accuracy: 0.6672\n","\n","Epoch 00006: val_loss improved from 3.00273 to 2.92442, saving model to Checkpoint.h5\n","Epoch 7/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.9092 - dice_coef: 0.4933 - accuracy: 0.6669 - val_loss: 2.8953 - val_dice_coef: 0.4910 - val_accuracy: 0.6672\n","\n","Epoch 00007: val_loss improved from 2.92442 to 2.89529, saving model to Checkpoint.h5\n","Epoch 8/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8851 - dice_coef: 0.4983 - accuracy: 0.6686 - val_loss: 2.8804 - val_dice_coef: 0.5017 - val_accuracy: 0.6672\n","\n","Epoch 00008: val_loss improved from 2.89529 to 2.88036, saving model to Checkpoint.h5\n","Epoch 9/100\n","426/426 [==============================] - 96s 225ms/step - loss: 2.8766 - dice_coef: 0.5029 - accuracy: 0.6667 - val_loss: 2.8764 - val_dice_coef: 0.5049 - val_accuracy: 0.6672\n","\n","Epoch 00009: val_loss improved from 2.88036 to 2.87640, saving model to Checkpoint.h5\n","Epoch 10/100\n"," 47/426 [==>...........................] - ETA: 1:19 - loss: 2.8697 - dice_coef: 0.5169 - accuracy: 0.6883"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XzwipgD8UuIy","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595563813221,"user_tz":-330,"elapsed":4582,"user":{"displayName":"Bhavishya Viswanath","photoUrl":"","userId":"05271986293024817587"}}},"source":["def stitchMaskPatches(pieces):\n","  k = 0\n","  reconstructed_img = np.ones([1440,1920])\n","  for r in range(6):\n","    row = r * 240\n","    for c in range(8):\n","      col = c * 240\n","      reconstructed_img[row:row+240,col:col+240] = pieces[k]\n","      k = k + 1\n","  return reconstructed_img\n","\n","\n","def stitchImagePatches(pieces):\n","  k = 0\n","  reconstructed_img = np.ones([1440,1920,3])\n","  for r in range(6):\n","    row = r * 240\n","    for c in range(8):\n","      col = c * 240\n","      reconstructed_img[row:row+240,col:col+240,:] = pieces[k]\n","      k = k + 1\n","  return reconstructed_img"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"0KNI0eyqvGcy","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595563813225,"user_tz":-330,"elapsed":4578,"user":{"displayName":"Bhavishya Viswanath","photoUrl":"","userId":"05271986293024817587"}}},"source":["def saveNumpyOutput(mask, Patient_array,Patient_length,title,folder):\n","  idx = 0\n","  for i in range(len(Patient_length)):\n","    temp = []\n","    for j in range(Patient_length[i]):\n","      final_output = mask[idx:idx+48]\n","      idx = idx + 48\n","      final_output = stitchMaskPatches(final_output)\n","      temp.append(final_output)\n","    final_output = np.asarray(temp)\n","    np.save(folder + title + Patient_array[i], final_output)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"NErfv7ZJ7Bxr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595563813734,"user_tz":-330,"elapsed":5059,"user":{"displayName":"Bhavishya Viswanath","photoUrl":"","userId":"05271986293024817587"}},"outputId":"60746d04-8fd4-49c3-fb53-b1319d3f8090"},"source":["MODELS_PATH = \"/content/drive/My Drive/Breast Cancer Treatment/Models/HER2/3Fold HER2NET/\"\n","model_names = os.listdir(MODELS_PATH)\n","model_names.sort()\n","print(model_names)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["['HER2NET_0.h5', 'HER2NET_1.h5', 'HER2NET_2.h5']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2bo0X0lXSwE6","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595563813739,"user_tz":-330,"elapsed":5055,"user":{"displayName":"Bhavishya Viswanath","photoUrl":"","userId":"05271986293024817587"}}},"source":["SAVE_PATH = \"/content/drive/My Drive/Breast Cancer Treatment/Numpy Arrays/Predicted Output/HER2/3Fold HER2NET/\""],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"B-zmdlCcbBGn","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595563924580,"user_tz":-330,"elapsed":1327,"user":{"displayName":"Bhavishya Viswanath","photoUrl":"","userId":"05271986293024817587"}}},"source":["# Axis 0 = Folds\n","# Axis 1 = Class (Membrane Nuclei Background)\n","# Axis 2 = Evaluation Technique (Precision, Recall, Dice coefficient)\n","pixEvaluationTrain = np.empty((0,3,3))\n","pixEvaluationTest = np.empty((0,3,3))\n","score = np.zeros((1,3,3))"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"wl31sylb0qvs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1595564797014,"user_tz":-330,"elapsed":867051,"user":{"displayName":"Bhavishya Viswanath","photoUrl":"","userId":"05271986293024817587"}},"outputId":"e96c4a1c-6369-4c61-a772-632f0ab9d2d3"},"source":["fold = 0\n","\n","for train_index, test_index in fold_split:\n","  print(\"FOLD:\",fold,\"\\tTRAIN:\", train_index, \"TEST:\", test_index)\n","  # Splitting Train and Test data\n","  TrainX =  np.concatenate(np.array([X[i] for i in train_index]))\n","  TrainY =  np.concatenate(np.array([Y[i] for i in train_index]))\n","  TestX  =  np.concatenate(np.array([X[i] for i in test_index]))\n","  TestY  =  np.concatenate(np.array([Y[i] for i in test_index]))\n","\n","  train_no = [patient_no[i] for i in train_index]\n","  train_size = [size[i] for i in train_index]\n","\n","  test_no = [patient_no[i] for i in test_index]\n","  test_size = [size[i] for i in test_index]\n","\n","  TrainX = TrainX.astype('float32')/255\n","  TestX = TestX.astype('float32')/255\n","\n","  convertToLabels(TrainY)\n","  convertToLabels(TestY)\n","\n","  # Loading corrosponding fold of model\n","  model = keras.models.load_model(MODELS_PATH + model_names[fold],custom_objects={ 'combined_loss': combined_loss, 'dice_coef': dice_coef })\n","\n","  # Predicting results using model\n","  trainResult = model.predict(TrainX, batch_size=8)\n","  testResult = model.predict(TestX,batch_size=8)\n","\n","  trainResult = np.argmax(trainResult,axis=-1)\n","  testResult = np.argmax(testResult,axis=-1)\n","\n","  # Obtaining pixel-wise results: Precision recall and dice coefficient\n","  # For TRAIN\n","  y_true = np.reshape(TrainY,(-1))\n","  y_pred = np.reshape(trainResult,(-1))\n","\n","  prec   = metrics.precision_score(y_true,y_pred,labels=[0,1,2],average=None,zero_division=1)\n","  recall = metrics.recall_score(y_true,y_pred,labels=[0,1,2],average=None,zero_division=1)\n","  dice = 2*prec*recall/(prec+recall)\n","\n","  # Precision Recall Dice for each class\n","  for i in range(3):\n","    score[0][i][0] = prec[2-i]\n","    score[0][i][1] = recall[2-i]\n","    score[0][i][2] = dice[2-i]\n","\n","  pixEvaluationTrain = np.append(pixEvaluationTrain,score,axis=0)\n"," \n","  # For TEST\n","  y_true = np.reshape(TestY,(-1))\n","  y_pred = np.reshape(testResult,(-1))\n","\n","  prec   = metrics.precision_score(y_true,y_pred,labels=[0,1,2],average=None,zero_division=1)\n","  recall = metrics.recall_score(y_true,y_pred,labels=[0,1,2],average=None,zero_division=1)\n","  dice = 2*prec*recall/(prec+recall)\n","\n","  # Precision Recall Dice for each class\n","  for i in range(3):\n","    score[0][i][0] = prec[2-i]\n","    score[0][i][1] = recall[2-i]\n","    score[0][i][2] = dice[2-i]\n","\n","  pixEvaluationTest = np.append(pixEvaluationTest,score,axis=0)\n","\n","  # Converting predicted labels to required output format\n","  convertFromLabels(trainResult)\n","  convertFromLabels(testResult)\n","\n","  # Saving numpy arrays\n","  path = os.path.join(SAVE_PATH,\"Fold \" + str(fold))\n","  os.mkdir(path)\n","  os.mkdir(path + '/Train')\n","  os.mkdir(path + '/Test')\n","\n","  saveNumpyOutput(trainResult, train_no, train_size,\"/Train/\",path)\n","  saveNumpyOutput(testResult, test_no, test_size,\"/Test/\",path)\n","\n","  fold +=1"],"execution_count":17,"outputs":[{"output_type":"stream","text":["FOLD: 0 \tTRAIN: [2, 3, 4, 5, 6, 7, 8] TEST: [0, 1]\n","FOLD: 1 \tTRAIN: [0, 1, 2, 4, 5, 7, 8] TEST: [3, 6]\n","FOLD: 2 \tTRAIN: [0, 1, 2, 3, 4, 6, 8] TEST: [5, 7]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IeQAQsZexovM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1595564798992,"user_tz":-330,"elapsed":1933,"user":{"displayName":"Bhavishya Viswanath","photoUrl":"","userId":"05271986293024817587"}},"outputId":"0b2cc3b0-5a6a-4942-a892-59a7cf2be9d1"},"source":["print(pixEvaluationTrain.shape)\n","print(pixEvaluationTest.shape)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["(3, 3, 3)\n","(3, 3, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"q9Z7ZVvgpEYc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1595564798994,"user_tz":-330,"elapsed":1906,"user":{"displayName":"Bhavishya Viswanath","photoUrl":"","userId":"05271986293024817587"}},"outputId":"4c550a5f-6eda-44f5-ba49-7eee16d11811"},"source":["print(pixEvaluationTest)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["[[[1.         0.         0.        ]\n","  [1.         0.         0.        ]\n","  [0.67794068 1.         0.80806275]]\n","\n"," [[1.         0.         0.        ]\n","  [1.         0.         0.        ]\n","  [0.65872666 1.         0.79425583]]\n","\n"," [[1.         0.         0.        ]\n","  [1.         0.         0.        ]\n","  [0.62069918 1.         0.7659647 ]]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZBNZbrAiyEhO","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595564798996,"user_tz":-330,"elapsed":1897,"user":{"displayName":"Bhavishya Viswanath","photoUrl":"","userId":"05271986293024817587"}}},"source":["np.save(SAVE_PATH + \"Pixel-wise Metrics Train.npy\", pixEvaluationTrain)\n","np.save(SAVE_PATH + \"Pixel-wise Metrics Test.npy\", pixEvaluationTest)"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZwjDP44MUTa6","colab_type":"text"},"source":["# **DISPLAY**"]},{"cell_type":"code","metadata":{"id":"DzUOdXOKU2nb","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import cv2\n","import numpy as np\n","from PIL import Image\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from google.colab.patches import cv2_imshow\n","\n","\n","plt.figure(figsize=(20,20))\n","id = 48   # enter between 0- 50 since there are 5 patients with 10 images each\n","\n","id = id * 48\n","final_input = TestX[id:id+48]\n","Mask_input = TestGT[id:id+48]\n","final_output = testResult[id:id+48]\n","\n","final_input = stitchImagePatches(final_input)\n","Mask_input =  stitchMaskPatches(Mask_input)\n","final_output = stitchMaskPatches(final_output)\n","\n","print(final_input.shape)\n","print(final_output.shape)\n","print(Mask_input.shape)\n","\n","print(np.unique(final_output))\n","copy1  = np.copy(final_input)\n","copy2 = copy1.astype('float32')*255\n","copy2 = copy2.astype('uint8')\n","final_input = np.reshape(copy2,(1440, 1920,3))\n","final_input = cv2.cvtColor(final_input,cv2.COLOR_BGR2RGB)\n","\n","plt.subplot(131).imshow(final_input)\n","plt.subplot(132).imshow(Mask_input,'gray')\n","plt.subplot(133).imshow(final_output,'gray')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t5IqVx01JeWn","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import cv2\n","import numpy as np\n","from PIL import Image\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from google.colab.patches import cv2_imshow\n","\n","\n","plt.figure(figsize=(20,20))\n","id = 25  # enter between 0- 40 since there are 4 patients with 10 images each\n","\n","id = id * 48\n","final_input = TrainX[id:id+48]\n","Mask_input = TrainGT[id:id+48]\n","final_output = trainResult[id:id+48]\n","\n","final_input = stitchImagePatches(final_input)\n","Mask_input =  stitchMaskPatches(Mask_input)\n","final_output = stitchMaskPatches(final_output)\n","\n","print(final_input.shape)\n","print(final_output.shape)\n","print(Mask_input.shape)\n","\n","print(np.unique(final_output))\n","copy1  = np.copy(final_input)\n","copy2 = copy1.astype('float32')*255\n","copy2 = copy2.astype('uint8')\n","final_input = np.reshape(copy2,(1440, 1920,3))\n","final_input = cv2.cvtColor(final_input,cv2.COLOR_BGR2RGB)\n","\n","plt.subplot(131).imshow(final_input)\n","plt.subplot(132).imshow(Mask_input,'gray')\n","plt.subplot(133).imshow(final_output,'gray')"],"execution_count":null,"outputs":[]}]}